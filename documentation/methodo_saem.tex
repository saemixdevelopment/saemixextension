
\chapter{Methodology and algorithms}  \label{chapter_methodology} \label{sec:methods}

\section{Estimation of the parameters}

\subsection{The SAEM algorithm}\label{saem}

We are in a classical framework of incomplete data: the observed data is $\yg=(y_{ij} \, ; \, 1  \leq i \leq N \ , 1 \leq j \leq n_i)$, whereas the random parameters $(\psig=\psigi$ \, ; \, $1 \leq i \leq N)$ are the non observed data. Then, the complete data of the model is $(\yg,\psig)$. Our purpose is to compute the maximum likelihood estimator of the unknown set of parameters $\thetag=(\fixed_effect,\IIV,\ag,\bg,\cg)$, by maximizing the likelihood of the observations $\ell(\yg ;\theta)$.

In the case of a linear model, the estimation of the unknown parameters can be treated with the usual EM algorithm. At iteration $k$ of EM, the E-step consists in computing the conditional expectation of the complete log-likelihood $Q_k(\theta)= \esp{\log p(\yg,\psig;\theta) | \yg,\theta_{k-1} }$ and the M-step consists in computing the value $\theta_{k}$ that maximises $Q_k(\theta)$.

Following \cite{Dempster77,Wu83}, the EM sequence $(\theta_k)$ converges to a stationary point of the observed likelihood ({\it i.e} a point where the derivative of $\ell$ is 0) under general regularity conditions. In cases  where the regression function $f$ does not linearly depend on the random  effects, the E-step cannot be performed in a closed-form.

The stochastic approximation version  of the  standard EM  algorithm, proposed by \cite{Delyon} consists in replacing the usual E-step of EM by a stochastic procedure. At iteration $k$ of SAEM:
\begin{itemize}
\item {\em Simulation-step} : draw $\psigk$ from the conditional distribution  $p(\cdot|\yg;\thetagk)$.
\item {\em Stochastic approximation} : update $Q_k(\theta)$ according to
\begin{equation}
 Q_k(\theta) = Q_{k-1}(\theta) + \gamma_k ( \log p(\yg,\psigk;\theta) - Q_{k-1}(\theta) )
\end{equation}
where $(\gamma_k)$ is a decreasing sequence of positive numbers with $\gamma_1=1$. 
\item {\em Maximization-step} : update $\thetagk$ according to 
$$\thetagkun={\rm Arg}\max_{\thetag} Q_k(\theta).$$
\end{itemize}


It is shown in \cite{Delyon} that SAEM converges to a maximum (local or global) of the likelihood of the observations under very general conditions.

Here, the complete log-likelihood can be written
\begin{eqnarray*}
\log p(\yg,\psig;\theta) & = &  \log p(\yg,h(\phig);\theta) \\
&=& - \sum_{i,j}\log( g(x_{ij},\psigi,\xi) )
-\frac{1}{2} \sum_{i,j}\left( \frac{y_{ij} - f(x_{ij},\psigi)}{g(x_{ij},\psigi,\xi) } \right)^2 \\
& & -\frac{N}{2} \log (|\IIV|) -\frac{1}{2}\sum_{i=1}^{N}(\phigi-\magi\fixed_effect)^\prime \IIV^{-1}(\phigi-\magi\fixed_effect)
 -\frac{ N_{tot}+Nd}{2}\log(2\pi)
\end{eqnarray*}
where $N_{tot}=\sum_{i=1}^{N}n_i$ is the total number of observations.

First, consider a constant residual error model ($g=a$). The set of parameters to estimate is $\theta=(\fixed_effect, \IIV, \ag)$. Then, the complete model belongs to the exponential family and the approximation step reduces to only updating the sufficient statistics of the complete model:
\begin{eqnarray*}
s_{1,i,k} &= & s_{1,i,k-1}  + \gamma_k \left(  \phig_{i,k} - s_{1,i,k-1}   \right) , \hspace{1em}i=1,\ldots,N \\
 s_{2,k} &= & s_{2,k-1}  + \gamma_k \left( \sum_{i=1}^{N} \phig_{i,k} \, \phig_{i,k}^\prime - s_{2,k-1}   \right)  \\
 s_{3,k} &= & s_{3,k-1}  + \gamma_k \left( \sum_{i,j} \left( y_{ij} - f(x_{ij},\psigki) \right) ^2      - s_{3,k-1} \right) .
% s_{3,k} &= & s_{3,k-1}  + \gamma_k \left( S_{3,k-1}   - s_{3,k-1} \right) .
\end{eqnarray*}
Then, $\theta_{k+1}$ is obtained in the maximization step as follows:
 \begin{eqnarray}
\mugkun &= &\left( \sum_{i=1}^{N}  \magi^\prime \IIVk ^{-1} \magi\right)^{-1} \sum_{i=1}^N \magi^\prime \IIVk ^{-1} s_{1,i,k}  \\
\IIVkun &=& \frac{1}{N} \left(s_{2,k}- \sum_{i=1}^N(\magi\mugkun)s_{1,i,k}^\prime -\sum_{i=1}^N s_{1,i,k}(\magi\mugkun)^\prime +\sum_{i=1}^N (\magi\mugkun)(\magi\mugkun)^\prime\right) \label{react_gamma} \\
\ag_{k+1} & = & \sqrt{ \frac{s_{3,k}}{N_{tot}} }
\end{eqnarray}


\noindent {\bf Remark 1:  } The sequence of step sizes used in \monolix~decreases as $k^{-a}$. More precisely, for any sequence of integers $K_1,K_2,\ldots,K_J$ and any sequence $a_1,a_2,\ldots,a_J$ of real numbers such that $0\leq a_1 <a_2<\ldots<a_J\leq 1$, we define the sequence of step sizes $(\gamma_k)$ as follows:
\begin{equation} \label{stepsize1}
\gamma_k = \frac{1}{k^{a_1}} \quad \mbox{for any } 1\leq k \leq K_1
\end{equation}
and for $2\leq j \leq J$,
\begin{equation} \label{stepsize2}
\gamma_k = \frac{1}{\left(k - K_{j-1}+\gamma_{K_{j-1}}^{-1/a_j}\right) ^{a_j}} \quad \mbox{for any } \sum_{i=1}^{j-1} K_i +1\leq k \leq \sum_{i=1}^{j} K_i
\end{equation}
Here, $K=\sum_{j=1}^{J}K_j$ is the total number of iterations.

We recommend to use $a_1=0$ (that is $\gamma_k=1$) during the first iterations, and $a_J=1$ during the last iterations. Indeed, the initial guess  $\theta_0$ may  be far  from the maximum likelihood value  we are looking for and the first iterations with $\gamma_k=1$ allow to converge quickly to a neighborhood of the maximum likelihood estimator. Then, smaller step sizes ensure the almost sure convergence of the algorithm to the maximum likelihood estimator.


\frame{\parbox{15cm} {\vspace*{.2cm} \quad In the case where $J=2$ with $a_1=0$ and $a_2=1$, the sequence of step sizes  is
\begin{eqnarray*}
\gamma_k &=& 1  \quad  \quad \quad \quad  \mbox{ for } 1\leq k \leq K_1 \\
 &=& \frac{1}{k-K_1+1}  \quad \mbox{ for } K_1+1 \leq  k \leq K_1+K_2
\end{eqnarray*}
}}


\noindent {\bf Remark 2:  } The estimated covariance matrix $\IIVkun$ defined in (\ref{react_gamma}) is a full covariance matrix. However, the covariance matrix $\IIV$ of the random effects can have any covariance structure. If we assume, for example, that there is no correlation between the random effects, we will set to 0 the non diagonal elements of $\IIVkun$ defined in (\ref{react_gamma}).

We can also assume that a random effect has no variance. If the $\ell$th random effect has a variance equal to 0, then the $\ell$th individual parameter is no longer random and the simulation step of SAEM needs some modification. During the first $K_0$ iterations, we use SAEM as it was described above, considering that all the effects are random and assuming that there is no correlation between the $\ell$th random effect and the other ones ($\omega^2_{\ell \ell^\prime}=0$ for any $\ell \neq \ell^\prime$). Then, during the next iterations, we use again SAEM, but the variance of this random effect is no longer estimated: it is forced to decrease at each iteration by setting
\begin{equation}
\omega^2_{\ell\ell,k+1}= \alpha \ {\omega^2_{\ell\ell,k}} \quad , \quad K_0 \leq k \leq K
\end{equation}
where $\alpha$ is chosen between 0 and 1 such that $\omega^2_{\ell\ell,K}= 10^{-6}\omega^2_{\ell\ell,K_0}$.

\noindent {\bf Remark 3:} - For a  residual variance model of the form $g=  \bg \, f^\cg$, where $\cg$ is fixed, the complete model also belongs to the exponential family and the estimation of $\bg$ is straightforward:  the sufficient statistics sequence $(s_{3,k})$ is defined by
$$ s_{3,k} =  s_{3,k-1}  + \gamma_k \left(
\sum_{i,j} \left( \frac{y_{ij} - f(x_{ij},\psigki)}{f^\cg (x_{ij},\psigki)}  \right) ^2  - s_{3,k-1}
\right) $$ and $\bg_{k+1} =  \sqrt{ s_{3,k}/ N_{tot} }$.

- For a general residual variance model $g= \ag + \bg \, f^\cg$, the complete model does not belong to the exponential family and the estimates of the residual variance parameters $(\ag,\bg,\cg)$ cannot be expressed as a function of some sufficient statistics. Then, let $(\Ag_k,\Bg_k,\Cg_k)$ that minimise the complete log-likelihood:
$$(A_k, B_k,C_k) = {\rm Arg}\min_{(\ag,\bg,\cg)}
\left\{ \sum_{i,j}\log( \ag + \bg f^{\cg}(x_{ij},\psigki) ) +\frac{1}{2} \sum_{i,j}\left(
\frac{y_{ij} - f(x_{ij},\psigki)}{\ag + \bg f^{\cg}(x_{ij},\psigki) } \right)^2 \right\}
$$
We update the residual variance parameters as follows:
 \begin{eqnarray}
\ag_{k+1} & = & \ag_{k} +  \gamma_k \left(  A_k - \ag_{k}  \right)   \\
\bg_{k+1} & = & \bg_{k} +  \gamma_k \left(  B_k - \bg_{k}  \right)   \\
\cg_{k+1} & = & \cg_{k} +  \gamma_k \left(  C_k - \cg_{k}  \right)
\end{eqnarray}
The estimation of $\fixed_effect$ and $\IIV$ remains unchanged.


\subsection{The MCMC-SAEM algorithm}
For model (1.1), the simulation step cannot be directly performed. Kuhn and Lavielle \cite{Kuhn01} propose to combine the SAEM algorithm with a MCMC (Markov Chain Monte Carlo) procedure. This procedure consists in replacing the Simulation-step at iteration $k$ by $m$ iterations of the Hastings-Metropolis algorithm.

Here, we will consider the Gaussian parameters $(\phigi)$. For $i=1,2,\ldots,N$
\begin{itemize}
\item let $\phig_{i,0}=\phig_{i}^{(k-1)}$
\item for $p=1,2,\ldots,m$,
\begin{enumerate}
\item draw $ \tilde{\phig}_{i,p}$ using the proposal kernel
$ q_{\theta_k}( \phig_{i,p-1} ,\cdot) $
\item set $ \phig_{i,p} =  \tilde{\phig}_{i,p} $ with probability
$$ \alpha( \phig_{i,p-1} ,  \tilde{\phig}_{i,p} ) = \min \left(1, \frac
{ p( \tilde{\phig}_{i,p} |y_i;\theta_k) q_{\theta_k}(\tilde{\phig}_{i,p} , \phig_{i,p-1} )}
{ p( \phig_{i,p-1} |y_i;\theta_k) q_{\theta_k}(\phig_{i,p-1} ,\tilde{\phig}_{i,p} )} \right)$$ and
$\phig_{i,p} =  \phig_{i,p-1}$ with probability $1- \alpha( \phig_{i,p-1} ,  \tilde{\phig}_{i,p} ) $.
\end{enumerate}
\item let $\phig_i^{(k)} =  \phig_{i,m}$.
 \end{itemize}

Several transition kernels, associated to different proposals can be successively used. We use the four following proposal kernels:

\begin{enumerate}
\item $q_{\theta_k}^{(1)}$  is the prior distribution of $\phig_i$ at iteration $k$, that is the Gaussian distribution \\
${\cal N}(C_i\fixed_effect_k,\IIV_k)$ and then
$$ \alpha( \phig_{i,p-1} ,  \tilde{\phig}_{i,p} ) = \min \left(1, \frac{ p( y_i|\tilde{\phig}_{i,p};\theta_k) } { p( y_i|\phig_{i,p-1};\theta_k) } \right)$$
\item $q_{\theta_k}^{(2)}$ is a random permutation of the $\phig_i$: generate a random permutation $\sigma$ of $\{1,2,\ldots,N\}$ and set $\tilde{\phig}_{i,p}=\phig_{\sigma(i),p-1}$.
\item $q_{\theta_k}^{(3)}$ is the multidimensional random walk ${\cal N}( \phig_{i,p-1} , \kappa\IIV_k)$. This kernel is symmetric and then 
$$ \alpha( \phig_{i,p-1} ,  \tilde{\phig}_{i,p} ) = \min \left(1, \frac{ p( y_i,\tilde{\phig}_{i,p};\theta_k) } { p( y_i,\phig_{i,p-1};\theta_k) } \right)$$
\item $q_{\theta_k}^{(4)}$ is a succession of $d$ unidimensional Gaussian random walks: each component of $\phig_i$ are successively updated.
\end{enumerate}
Then, the simulation-step at iteration $k$ consists in running $m_1$ iterations of the Hasting-Metropolis with proposal $q_{\theta_k}^{(1)}$, $m_2$ iterations with proposal $q_{\theta_k}^{(2)}$, $m_3$ iterations with proposal $q_{\theta_k}^{(3)}$ and $m_4$ iterations with proposal $q_{\theta_k}^{(4)}$.


\noindent {\bf Remark 1 : \ } During the first $K_b$ iterations ("burning" iterations) of SAEM, we only run the MCMC algorithm but the parameters are not updated.

 \noindent {\bf Remark 2 : \ } When the number $N$ of subjects is small, convergence of the algorithm can be improved by running $L$ Markov Chain instead of only one. The simulation step requires to draw $L$ sequences $ { \phig^{(k,1)}} ,\ldots , { \phig^{(k,L)}} $ at iteration $k$ and to combine stochastic approximation and Monte Carlo in the approximation step:
\begin{equation} \label{approx2}
 Q_k(\theta) = Q_{k-1}(\theta) + \gamma_k \left( \frac{1}{L}\sum_{\ell=1}^{L} \log p(\yg, { \phig^{(k,\ell)}} ;\theta) - Q_{k-1}(\theta) \right)
\end{equation}


\subsection{The Simulated Annealing SAEM algorithm}
Convergence of SAEM can strongly depend on the initial guess if the likelihood $\ell$ possesses several local maxima. The Simulated Annealing version of SAEM improves the convergence of the algorithm toward the global maximum of $\ell$.

For the sake of simplicity, we will consider here a constant residual error model $g=\ag$. Let 
$$ U(\yg,\phig;\theta) = \frac{1}{2\ag^2} \sum_{i,j}\left(y_{ij} - f(x_{ij},h(\phigi)) \right)^2 +\frac{1}{2}\sum_{i=1}^{N}(\phigi-\magi\fixed_effect)^\prime \IIV^{-1}(\phigi-\magi\fixed_effect)$$ 
Then, we can write the complete likelihood:
\begin{eqnarray*}
p(\yg,\phig;\theta)  =  C(\theta)\, e^{-U(\yg,\phig;\theta)}
\end{eqnarray*}
where $C(\theta)$ is a normalizing constant that only depends on $\theta$.

For any {\it temperature} $T\geq0$, we consider the complete model
\begin{eqnarray*}
p_T(\yg,\phig;\theta)  =  C_T(\theta)\, e^{-\frac{1}{T}U(\yg,\phig;\theta)}
\end{eqnarray*}
where $C_T(\theta)$ is a normalizing constant. This model consists in replacing the variance matrix $\IIV$ by $T\IIV$ and the residual variance $\ag^2$ by $T\ag^2$. In other words, a model ``with a large temperature'' is a model with large variances.

We introduce a decreasing temperature sequence $(T_k, 1\leq k \leq K)$ and use the MCMC-SAEM algorithm considering the complete model $p_{T_k}(\yg,\phig;\theta)$  at iteration $k$ (while the usual version of MCMC-SAEM uses $T_k=1$ at each iteration). The sequence $(T_k)$ is large during the first iterations and decreases to 1 with exponential rate. This is done by choosing large initial variances $\IIV_0$ and $\ag^2_0$ and setting
\begin{eqnarray}
 \tilde{\IIV}_{k+1} &=& \frac{1}{N} \left(s_{2,k}- \sum_{i=1}^N(\magi\mugkun)s_{1,i,k}^\prime -\sum_{i=1}^N s_{1,i,k}(\magi\mugkun)^\prime +\sum_{i=1}^N (\magi\mugkun)(\magi\mugkun)^\prime\right)  \\
\ag_{k+1} & = & \sqrt{ \frac{s_{3,k}}{N_{tot}} }\\
\IIV_{k+1} &=& \max\left( \tau \IIV_{k}  \ , \ \tilde{\IIV}_{k+1}  \right)  \label{tau1} \\
\ag^2_{k+1} & = &  \max\left( \tau \ag^2_{k} \ , \  \frac{s_{3,k}}{N} \right) \label{tau2}
\end{eqnarray}
%\begin{eqnarray}
% \IIV_k  & = &  \tau \IIV_{k-1} ,  \label{tau1} \\
%\ag^2_k & = & \tau \ag^2_{k-1} \label{tau2}
%\end{eqnarray}
during the first iterations of the algorithm and where $0\leq\tau\leq 1$.

These large values of the variances make the conditional distribution $p(\phi|y;\theta )$ less concentrated around its mode. This procedure allows the sequence $(\theta_k)$ to escape from the local maxima of the likelihood and to converge to a neighborhood of the global maximum of $\ell$. After that, the usual MCMC-SAEM algorithm is used, estimating the variances at each iteration.


\noindent {\bf Remark 1:} The Simulated Annealing version of SAEM is performed during the first $K_{sa}$ iterations. Of course, SAEM without any simulated annealing can be run by setting $\tau=0$. On the other hand, simulated annealing is obtained with $\tau$ close to 1.

\noindent {\bf Remark 2:} We can use two different coefficients $\tau_1$ and $\tau_2$ for $\IIV$ and $\ag^2$ in \monolix. It is possible, for example, to choose $\tau_1<1$ and $\tau_2>1$, with a small initial residual variance and large initial inter-subject variances. In this case, SAEM tries to obtain the best possible fit during the first iterations, allowing a large inter-subject variability. During the next iterations, this variability is reduced and the residual variance increases until reaching the best possible trade-off between these two criteria.

\subsection{The MCMC-SAEM algorithm for non continuous data models}

As presented Section~\ref{sec:longitudinal}, the model between some measured observations noted $y_{ij}$, the individual parameters $\psi_i$ and the design variables $x_{ij}$ can either be defined by a structural model, for \emph{continuous} data models, i.e. when $y_{ij}$ are continuous, noted $f(x_{ij}, \psi_i)$, or defined by a conditional probability $\prob(y_{ij}=c_k|\psi_i)$ for \emph{non continuous} data models, i.e. when $y_{ij}$ takes discrete values $\{c_1, c_2,\ldots , c_K\}$. Such models are presented Section~\ref{sec:longitudinal} and include for instance count or categorical models.

For both cases, the MCMC-SAEM algorithm described in the previous sections remain unchanged, the only difference lies on the way the model is defined: using a structural and continuous function for the former and a conditional probability for the latter. From an implementation perspective while using saemix, this plays an important role as the user needs to define the model itself.

\subsection{A fast variant of the MCMC-SAEM algorithm for general data models}\label{subsec:fsaem}

We propose in this section a variant of the MCMC-SAEM algorithm, called f-SAEM, see \cite{Karimi19}, and derived for both continuous and non continuous data models.
According to the definition of the model, the algorithm presents a slight variation.
The whole purpose of the f-SAEM is to accelerate the convergence of the MLE estimation leveraging a simple and efficient new MCMC proposal.
This proposal is obtained via two different techniques depending on the nature of the model.

\textbf{For continuous data models}, we linearize the structural model $f(x_{i}, \psi_i)$ around a particular point, $\hat{\psi}_i$, which is the MAP (Maximum A Posteriori) of the conditional distributions of the individual parameters $\psi_i$ given the observations noted $p(\psi_i|y_i) $. This particular point is tractable since we have the following identity, by Bayes rule:

\begin{equation}
\arg \max \limits_{\psi_i \in \mathbb{R}^p} p(\psi_i|y_i) = \arg \max \limits_{\psi_i \in \mathbb{R}^p} p(y_i|\psi_i)p(\psi_i) \eqs.
\end{equation}

The Taylor expansion of $f(x_{i},\cdot)$ around the MAP yields:

\begin{equation}
f(x_{i},\psi_i) \approx f(x_{i},\hat{\psi}_i) + J_{f(x_{i},\hat{\psi}_i)}(\psi_i - \hat{\psi}_i)\eqs,
\end{equation}
where $J_{f(x_{i},\hat{\psi}_i)} \in \mathbb{R}^{n_i \times p}$ is the Jacobian of $f$ evaluated at $\hat{\psi}_i$. 

Remark that the conditional distribution of the individual parameters given the observations under this linearized model is tractable and is a Gaussian distribution of covariance:


\begin{equation}
\Gamma_i =\left(\frac{ J_{f(x_{i},\hat{\psi}_i)}^T J_{f(x_{i},\hat{\psi}_i)} }{\sigma^2} + \Omega^{-1}\right)^{-1} \eqs,
\end{equation}
where $\sigma$ is the residual error of the continuous data model and $\Omega$ is the covariance of the prior distribution $p(\psi_i)$.

Hence, the f-SAEM algorithm uses the Gaussian proposal of covariance $\left(\frac{ J_{f(x_{i},\hat{\psi}_i)}^T J_{f(x_{i},\hat{\psi}_i)} }{\sigma^2} + \Omega^{-1}\right)^{-1}$ and  mean $\hat{\psi}_i$, as an approximation of the true conditional distribution, in the MCMC sampler for continuous data models.


\textbf{For non continuous data models}, we use a Laplace approximation of the conditional distribution obtained via a Taylor expansion of the complete log-likelihood $\log p(y_i,\psi_i)$ of the observations $y_i$ and individual parameters $\psi_i$.
A few derivation of this expansion leads to the following approximation:

$$
-2\log p(y_i,\psi_i)  \approx -p\log2\pi - 2\log p(y_i,\hat{\psi}_i) + \log \left(\left|-\nabla^2 \log p(y_i,\hat{\psi}_i)\right|\right)\eqs.
$$

and finally:

 $$
\log p(\hat{\psi}_i|y_i) \approx -\frac{p}{2}\log2\pi  -\frac{1}{2}\log \left(\left|-\nabla^2 \log p(y_i,\hat{\psi}_i) \right|\right)\eqs,
$$
which is precisely the log-pdf of a multivariate Gaussian distribution with mean $\hat{\psi}_i$ and  variance-covariance $-\nabla^2 \log p(y_i,\hat{\psi}_i)^{-1}$ where:

\begin{align}
\nabla^2 \log p(y_i,\hat{\psi}_i) &= \nabla^2 \log p(y_i|\hat{\psi}_i) + \nabla^2 \log p(\hat{\psi}_i) \\ \notag
&= \nabla^2 \log p(y_i|\hat{\psi}_i) + \Omega^{-1}\eqs.\notag
\end{align}

Hence, the f-SAEM algorithm uses the Gaussian proposal of covariance $\left( \nabla^2 \log p(y_i|\hat{\psi}_i) + \Omega^{-1}\right)^{-1}$ and mean $\hat{\psi}_i$, as an approximation of the true conditional distribution, in the MCMC sampler for non continuous data models.

\section{Estimation of the Fisher Information matrix} \label{sec_fish} Let $\thes$ be the true unknown value of $\theta$, and let $\hthetag$ be the maximum likelihood estimate of $\theta$. If the observed likelihood function $\ell$ is sufficiently smooth, asymptotic theory for maximum-likelihood estimation holds and

\begin{equation}
\sqrt{N}(\hthetag-\thes) \limite{N\to \infty}{} {\mathcal N}(0,I(\thes)^{-1})
\end{equation}
where $I(\thes)=- \DDt \log \ell(y;\thes)$ is the true Fisher information matrix. Thus, an estimate of the asymptotic covariance of $\hthetag$ is the  inverse of the Fisher information matrix $I(\hthetag)=- \DDt \log \ell(y;\hthetag)$.

\subsection{Linearization of the model}
The Fisher information matrix of the non-linear mixed effects model defined in (1) cannot be computed in a closed-form.

An alternative is  to approximate this information matrix  by the Fisher information matrix of the Gaussian model deduced from the non-linear mixed effects model after linearization of the function $f$ around the conditional expectation of the individual Gaussian parameters $(\esp{\phi_{i}|y;\hat{\theta}}, 1\leq
i \leq N) $. The Fisher information matrix of this Gaussian model is a block matrix (no correlations between the estimated fixed effects and the estimated variances). The gradient of $f$ is numerically computed.

\noindent {\bf Remark 1: } We do not recommend the linearization of the model to estimate the parameters of the model, as it is done with the FO and FOCE algorithms. On the other hand, many numerical experiments have shown that this approach can be used to estimate the Fisher information matrix.

\noindent {\bf Remark 2: } Obviously, this approach cannot be used with discrete data models\ldots

\subsection{A stochastic approximation of the Fisher Information Matrix}
It is possible to obtain an estimation of the Fisher information matrix using the Louis's missing information principle \cite{Louis82}:

\begin{equation}\label{louis}
\DDt \log \ell(y;\theta) = \mathrm{E}\big(\DDt \log p(y,\phig; \theta) | y ; \theta \big) +
\mathrm{Cov}\big(\Dt \log p(y,\phig;\theta)| y ; \theta \big)
\end{equation}
where
\begin{align*} \quad
\mathrm{Cov}\big(\Dt \log p(y,\phig;\theta)| y ; \theta \big) &=
\mathrm{E} \big(\Dt \log p(y,\phig; \theta) \Dt \log p(y,\phig; \theta)^\prime |y ; \theta \big)  \\
& -
\mathrm{E} \big( \Dt \log p(y,\phig; \theta) |y ; \theta \big)\mathrm{E} \big( \Dt \log p(y,\phig; \theta) |y ; \theta \big)^\prime
\end{align*}
and
$$\Dt \log g(y;\theta) = \mathrm{E} \big( \Dt \log p(y,\phig; \theta) |y ; \theta \big)$$
Here, $\Dt u$ is the gradient of $u$ ({\it i.e.} the vector of first derivatives of $u$ with respect to $\theta$) and  $\DDt u$ is the hessian of $u$ ({\it i.e.} the matrix of second derivatives of $u$ with respect to $\theta$).

Then, using SAEM, the matrix $\DDt \log \ell (y;\hthetag)$ can be approximated by the sequence $(H_k)$ defined as follows:
\begin{align*}
\Delta_k&=\Delta_{k-1}+\gamma_k\left(\partial_{\theta}\log
f(y,\phi_k;\theta_k)-\Delta_{k-1}\right)\\
D_k  &=   D_{k-1}   +   \gamma_k  \left( \DDt \log  f (y,  \phi_k;
    \theta_k)   - D_{k-1} \right) \\
G_k  &=   G_{k-1}   +   \gamma_k  \left(  \Dt \log f(y, \phi_k; \theta_k) \Dt \log f(y, \phi_k; \theta_k)^t - G_{k-1} \right) \\
H_{k} &= D_k + G_k - \Delta_k \Delta_k^t
\end{align*}

In the current version of \monolix, only the linearisation approach has been implemented.

\section{Estimation of the individual parameters} \label{section_indivparam}

When the parameters of the model have been estimated, we can estimate the individual parameters $(\psigi)$. To do that, we will estimate the individual normally distributed parameters $(\phigi)$ and derive the estimates of $(\psigi)$ using the transformation $\psigi= h(\psigi)$.

Let $\hat{\theta}$ be the estimated value of $\theta$ computed with the SAEM algorithm and let $p(\phigi|y_i;\hat{\theta})$ be the conditional distribution of $\phigi$ for $1\leq i \leq N$.

We use the MCMC procedure used in the SAEM algorithm to estimate these conditional distributions. More precisely, for $1\leq i \leq N$, we empirically estimate:

\begin{itemize}
  \item the conditional mode (or Maximum A Posteriori)
  $ m(\phigi|y_i;\hat{\theta})={\rm Arg}\max_\phigi p(\phigi|y_i;\hat{\theta})$,
  \item the conditional mean
  $ E(\phigi|y_i;\hat{\theta})$,
  \item the conditional standard deviation
 $ sd(\phigi|y_i;\hat{\theta})$.
 \end{itemize}

 \noindent{\bf Remarks:}
 \begin{enumerate}
  \item The prior distribution of $\phigi$ is a normal distribution, but not the conditional distribution $p(\phigi|y_i;\hat{\theta})$ (remember that the structural model is not a linear function of $\phigi$\ldots). Then, the conditional mode $m(\phigi|y_i;\hat{\theta})$ and the conditional expectation $ E(\phigi|y_i;\hat{\theta})$ are two different predictors of $\phigi$.
  \item If the transformation $h$ is not linear,
      \begin{eqnarray*}
       \esp{\psigi|y_i;\hat{\theta}} &=&  \esp{h(\phigi|y_i;\hat{\theta}} \\
       &\neq& h\left(\esp{\phigi|y_i;\hat{\theta}}  \right)
       \end{eqnarray*}
       In \monolix, we estimate $\esp{\phigi|y_i;\hat{\theta}}$ and $\esp{\psigi|y_i;\hat{\theta}}$.
\end{enumerate}

The number of iterations of the MCMC algorithm used to estimate the conditional mean and standard deviation is adaptively chosen as follows:
\begin{enumerate}
  \item the $(\phig_i)$ are initialised with the last value obtained in SAEM
  \item we run the Hastings-Metropolis with kernel $q^{(1)}$, $q^{(3)}$ and $q^{(4)}$ and compute at each iteration the empirical conditional mean and s.d. of $\phig_i$:
  \begin{eqnarray}
  e_{i,K} &= &\frac{1}{K}\sum_{k=1}^K \phig_{i,k} \\
  sd_{i,K} &= &\sqrt{\frac{1}{K}\sum_{k=1}^K \phig_{i,k}^2 - e_{i,K}^2 }
  \end{eqnarray}
  where $\phig_{i,k}$ is the value of $\phig_i$ at iteration $k$ of the MCMC algorithm.
  \item we stop the algorithm at iteration $K$ and use $e_{i,K}$ and $sd_{i,K}$ to estimate the conditional mean and s.d. of $\phig_i$ if, for any $ K-L_{mcmc}+1 \leq k \leq K$,
  \begin{eqnarray}
  % \nonumber to remove numbering (before each equation)
  \label{Lmcmc}  (1-\rho_{mcmc})\bar{e}_K & \leq & \bar{e}_k \leq  (1+\rho_{mcmc})\bar{e}_{K} \\
  \nonumber  (1-\rho_{mcmc})\bar{sd}_{K} & \leq & \bar{sd}_{k} \leq  (1+\rho_{mcmc})\bar{sd}_{K}
  \end{eqnarray}
where $0<\rho_{mcmc}<1$. That means that the sequence of empirical means and s.d. must stay in a $\rho_{mcmc}$-confidence interval during $L_{mcmc}$ iterations.
\end{enumerate}

\section{Estimation of the likelihood} \label{estilik}
\subsection{Linearization of the model}
The likelihood of the non-linear mixed effects model defined in (1) cannot be computed in a closed-form.

An alternative is  to approximate this likelihood  by the likelihood of the Gaussian model deduced from the non-linear mixed
effects model after linearization of the function $f$ around
the predictions of the individual parameters $(\phigi, 1\leq i \leq N) $.
%the conditional expectation of the individual parameters $(\esp{\phig_{i}|y;\hat{\theta}}, 1\leq i \leq N) $.

\subsection{Estimation using importance sampling}

The likelihood of the observations can be estimated without any approximation using a Monte-Carlo approach. The likelihood $\ell$ of the observations can be decomposed as follows:
\begin{eqnarray*}
 \ell(\yg;\thetag) &=& \int p(\yg,\phig;\thetag)\,d\phig \\
&=& \int h(\yg|\phig;\thetag)\pi(\phig;\thetag)\,d\phig
\end{eqnarray*}
where $\pi$ is the so-called {\it prior distribution} of $\phig$. According to (\ref{prior}), $\pi$ is a Gaussian distribution.

For any distribution $\tilde{\pi}$ absolutely continuous with respect to the prior distribution $\pi$, we can write
$$
\ell(\yg;\thetag) = \int h(\yg|\phig;\thetag) \frac {\pi(\phig;\thetag)}{
\tilde{\pi}(\phig;\thetag) }  \tilde{\pi}(\phig;\thetag) \,d\phig
$$


Then, $\ell(\yg;\thetag)$ can be approximated via an {\it Importance Sampling} integration method:
\begin{enumerate}
\item draw $\phig^{(1)} ,\phig^{(2)} ,\ldots,\phig^{(M)} $ with the distribution $\tilde{\pi}(\cdot;\thetag)$,
\item let
\begin{equation} \label{islike}
 \ell_M(\yg;\thetag) =\frac{1}{M} \sum_{j=1}^{M} h(y|\phig^{(j)};\thetag )\frac{\pi( \phig^{(j)} ;\thetag)}{ \tilde{\pi}(\phig^{(j)} ;\thetag) }
\end{equation}
\end{enumerate}

The statistical properties of the estimator $\ell_M(\yg;\thetag)$ of the likelihood
$\ell(\yg;\thetag)$ strongly depend on the sampling distribution  $\tilde{\pi}$. First, note that
\begin{eqnarray*}
\esp{{\ell}_M(\yg;\thetag)} &=& \ell(\yg;\thetag), \\
\var{{\ell}_M(\yg;\thetag)} &=& {\cal O}(1/M).
\end{eqnarray*}
Furthermore, if $\tilde{\pi}$ is the conditional distribution $p(\phi|\yg;\thetag)$, the variance of the estimator is null and $\hat{\ell}_M(\yg;\thetag) = \ell(\yg;\thetag)$ for any value of $M$. That means that an accurate estimation of $\ell(\yg;\thetag)$ can be obtained with a small value of $M$ if the sampling distribution is close to the conditional distribution $p(\phi|\yg;\thetag)$.

In \monolix, for $i=1,2,\ldots, N$, we empirically estimate the conditional mean $\esp{\phigi|y_{i};\hthetag}$ and the conditional variance $\var{\phigi|y_{i};\hthetag}$ of $\phigi$ as described above. Then, the $\phigi^{(j)}$ are drawn with the sampling distribution $\tilde{\pi}$ as follows:
$$\phigi^{(j)} = \esp{\phigi|y_{i};\hthetag} + \var{\phigi|y_{i};\hthetag}^{\frac{1}{2}} \times T_{ij}$$
where $(T_{ij})$ is a sequence of {\it i.i.d.} random variables distributed with a $t-$distribution with $\nu$ degrees of freedom. In the current version of \monolix, the default value is $\nu=5$. 

%It is also possible to automatically test different d.f in $\{2, 5, 10, 20\}$ and to select the one that provides the smallest empirical variance for $\ell_M(\yg;\thetag)$.
% ECO TODO: à faire dans une version utltérieure

The quality of the approximation depends on the estimates of the conditional mean and variances of the individual distributions.

\subsection{Estimation using Gaussian Quadrature} \label{sec:gqlike}

% ECO TODO
% see nia06_GHQ.pdf, hartford00

Gauss-Hermite quadrature methods use a fixed set of $K_{GQ}$ ordinates (called nodes) and weights $(x_k, w_k)_{k=1,...,K_{GQ}}$ to approximate the likelihood function.

As for importance sampling, the quality of the approximation depends on the estimates of $\esp{\phigi|y_{i};\hthetag}$ and $\var{\phigi|y_{i};\hthetag}$.

% Adaptive Gaussian quadrature are an alternative numerical approach that centres the Gaussian approximation to the likelihood at the posterior mode of the random effects. Although each integral takes longer to compute, the gain in accuracy over standard Gaussian quadrature can be very large.

\section{Model predictions} \label{section_preds}

\subsection{Population predictions}

Population predictions represent the predictions from the model in the absence of data, and they only take into account individual design variables (eg dose regimen) and covariates.

Two types of population predictions are available in \monolix:
\begin{enumerate}
  \item the predictions using the population parameters: $f\left(x_{ij} ; h\left( \mathbb{E}_{\hat{\theta}}(\phigi) \right) \right)= f(x_{ij} ; h(C_i \hat{\fixed_effect}))$. These are provided in the output under the name \texttt{ppred}
  \item the population mean predictions:  $\mathbb{E}_{\hat{\theta}}(f(x_{ij} ; \psigi) ))=\mathbb{E}_{\hat{\theta}}(f(x_{ij} ; h(\phigi) ))$. These are provided in the output under the name \texttt{ypred}
\end{enumerate}

\subsection{Individual predictions}

Individual predictions take into account not only covariates and individual design variables such as dose regimen, but also use the observations in that individual to obtain the parameters providing the best fit for that particular subject, given the population parameters.

In section~\ref{section_indivparam}, we described how the conditional distribution of the parameters for each individual is obtained in \monolix. Two types of individual parameters are reported in the output:
\begin{enumerate}
\item the conditional mode (or Maximum A Posteriori): $ m(\phigi|y_i;\hat{\theta})={\rm Arg}\max_\phigi p(\phigi|y_i;\hat{\theta})$. These are reported in the output as \texttt{map.psi}
\item the conditional mean: $ E(\phigi|y_i;\hat{\theta})$. These are reported in the output as \texttt{cond.mean.psi}
\end{enumerate}
Correspondingly, two types of individual predictions can be obtained in \monolix:
\begin{enumerate}
\item the predictions obtained using the conditional mode are reported in the output as \texttt{ipred}
\item the predictions obtained using the conditional mean are reported in the output as \texttt{icpred}
\end{enumerate}

\section{Estimation of the weighted residuals} \label{section_wres}

\subsection{Population Weighted Residuals}
The vector of Population Weighted Residuals are evaluated as:
$$PWRES_{i} = Var_{\hat{\theta}}(y_{i})^{-1/2} \left( y_{i} - \hat{y}^{pop}_{i} \right)$$
where $\hat{y}^{pop}_{ij}$ is the population prediction of $y_{ij}$ and $Var_{\hat{\theta}}(y_{ij})$ is the variance-covariance matrix of $y_{i}$.

In \monolix, weighted residuals are computed using the population mean predictions \texttt{ypred} $\mathbb{E}_{\hat{\theta}}(f(x_{ij} ; \psigi) ))=\mathbb{E}_{\hat{\theta}}(f(x_{ij} ; h(\phigi) ))$ for $\hat{y}^{pop}_{ij}$. $\mathbb{E}_{\hat{\theta}}(f(x_{ij} ; h(\phigi)  )$ and $Var_{\hat{\theta}}(y_{ij})$ are estimated with a Monte-Carlo procedure.

\noindent {\bf Remark:} This computation is performed during the computation of pd/npde, so that a basic \monolix~object does not include these elements.

% ECO TODO: voir quelle version je calcule... idem pour IWRES bien préciser

\subsection{Individual Weighted Residuals}

The Individual Weighted Residuals are evaluated as
$$IWRES_{ij} = \frac{y_{ij} - \hat{y}^{ind}_{ij}} {\hat{\sigma}^{ind}_{ij}}$$
where  $\hat{y}^{ind}_{ij} = f(x_{ij} ; \hat{\psig}_i )$ is the individual prediction of $y_{ij}$ and $(\hat{\sigma}^{ind}_{ij})^2 = g(x_{ij} ; \hat{\phig}_i,\hat{\xi})^2$ is the residual variance of $y_{ij}$.

The two types of individual parameters described in section~\ref{section_preds} yield two types of individual weighted residuals in the \monolix~output:
\begin{enumerate}
\item the individual weighted residuals obtained using the conditional mode are reported in the output as \texttt{iwres}
\item the individual weighted residuals obtained using the conditional mean are reported in the output as \texttt{icwres}
\end{enumerate}

\noindent {\bf Remark:} When a transformed residual error model is used (an exponential error model for instance), the weighted residuals are computed using $t(y)$ instead of $y$.

\subsection{Normalised Prediction Distribution Errors}

The Normalised Prediction Distribution Errors are defined as follow
$$\npde_{ij}=\Phi^{-1}(\hat{p}_{ij})$$
where $\Phi$ is the $\Nr(0,1)$ cumulative distribution function and where $\hat{p}_{ij}$ is an empirical estimator of
$$p_{ij} = \Prob{Y_{ij}<y_{ij}}$$
obtained by Monte-Carlo. 

In more details, prediction discrepancies ($\pd$) are first obtained, as the percentile of the observation in the cumulative distribution function $F_{ij}$ of the predictive distribution of $Y_{ij}$ under the model being evaluated. $F_{ij}$ is obtained by simulating $K$ datasets under the model, and the corresponding prediction discrepancies is given by:
\begin{equation}
\pd_{ij} = F_{ij}(y_{ij}) \approx \frac{1}{K} \sum_{k=1}^K \delta_{ijk} \label{eq:pdedef}
\end{equation}
where $\delta_{ijk}=1$ if $y_{ij}^{sim(k)} < y_{ij}$ and 0 otherwise, $y_{ij}^{sim(k)}$ denoting the value of $y_{ij}$ simulated in the k$^{\rm th}$ replication.

To handle correlations within the observations obtained in the same individual, we first compute the empirical mean $\hat{\E}(\y_i)$ and empirical variance-covariance matrix $\var(\y_i)$ over the $K$ simulations. Decorrelation is performed simultaneously for simulated data:
\begin{equation}
\y_{i}^{sim(k)*}= \hat{\V}_i^{-1/2} (\y_{i}^{sim(k)}-\hat{\E}(\y_i)) \label{eq:decorrsim}
\end{equation}
and for observed data:
\begin{equation}
\y_{i}^*= \hat{\V}_i^{-1/2} (\y_{i}-\hat{\E}(\y_i)) \label{eq:decorrobs}
\end{equation}

Decorrelated $\pd$ are then obtained using the same formula as in~(\ref{eq:pdedef}) but with the decorrelated data, and we call the resulting variables prediction distribution errors ($\pde$):
\begin{equation}
\pde_{ij} = F^*_{ij}(y^*_{ij}) \approx \frac{1}{K} \sum_{k=1}^K \delta_{ijk}^*
\end{equation}
where $\delta_{ijk}^*=1$ if $y_{ij}^{sim(k)*} < y_{ij}^*$ and 0 otherwise.

Normalised prediction distribution errors ($\npde$) are then obtained as:
\begin{equation}
\npde_{ij} = \Phi^{-1} (\pde_{ij})
\end{equation}

\begin{description}
\item[Remark:] the empirical mean and covariance-matrix computed here are also used for the decorrelation step in the computation of the population weighted residuals, $WRES$. The $WRES$ in \monolix ~are thus computed in conjunction with the more advanced metric $\npde$.
\end{description}

