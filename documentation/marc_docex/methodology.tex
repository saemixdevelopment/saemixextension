
\chapter{Methodology and algorithms}  \label{chapter_methodology}
\section{Estimation of the parameters}
\subsection{The SAEM algorithm}\label{saem}

We are in a classical framework of incomplete data: the observed
data is $\yg=(y_{ij} \, ; \, 1  \leq i \leq N \ , 1 \leq j \leq
n_i)$, whereas the random parameters $(\psig=\psigi$ \, ; \, $1 \leq
i \leq N)$ are the non observed data. Then, the complete data of the
model is $(\yg,\psig)$. Our purpose is to compute the maximum
likelihood estimator of the unknown set of parameters
$\thetag=(\fixed_effect,\IIV,\ag,\bg,\cg)$, by maximizing the
likelihood of the observations $\ell(\yg ;\theta)$.

In the case of a linear model, the estimation of the unknown parameters can be treated with the
usual EM algorithm. At iteration $k$ of EM, the E-step consists in computing the conditional
expectation of the complete log-likelihood $Q_k(\theta)= \esp{\log p(\yg,\psig;\theta) |
\yg,\theta_{k-1} }$ and the M-step consists in computing the value $\theta_{k}$ that maximizes
$Q_k(\theta)$.

Following \cite{dempster,wu}, the EM sequence $(\theta_k)$ converges
to a stationary point of the observed likelihood ({\it i.e} a point
where the derivative of $\ell$ is 0) under general regularity
conditions. In cases  where the regression function $f$ does not
linearly depend on the random  effects, the E-step cannot be
performed in a closed-form.

The stochastic approximation version  of the  standard EM  algorithm, proposed by \cite{marc}
 consists in replacing the usual E-step of EM by a stochastic
procedure. At iteration $k$ of SAEM:
\begin{itemize}
\item {\em Simulation-step} : draw $\psigk$ from the
conditional distribution  $p(\cdot|\yg;\thetagk)$.
\item {\em Stochastic approximation} : update $Q_k(\theta)$ according to
\begin{equation}
 Q_k(\theta) = Q_{k-1}(\theta) + \gamma_k ( \log p(\yg,\psigk;\theta) - Q_{k-1}(\theta) )
\end{equation}
where $(\gamma_k)$ is a decreasing sequence of positive numbers with $\gamma_1=1$.
\item {\em Maximization-step} : update $\thetagk$ according to
$$\thetagkun={\rm Arg}\max_{\thetag} Q_k(\theta).$$

\end{itemize}


It is shown in \cite{marc} that
 SAEM converges to a maximum (local or global) of the likelihood of the observations under very general conditions.

Here, the complete log-likelihood can be written
\begin{eqnarray*}
\log p(\yg,\psig;\theta) & = &  \log p(\yg,h(\phig);\theta) \\
&=& - \sum_{i,j}\log( g(x_{ij},\psigi,\xi) )
-\frac{1}{2} \sum_{i,j}\left( \frac{y_{ij} - f(x_{ij},\psigi)}{g(x_{ij},\psigi,\xi) } \right)^2 \\
& & -\frac{N}{2} \log (|\IIV|) -\frac{1}{2}\sum_{i=1}^{N}(\phigi-\magi\fixed_effect)^\prime \IIV^{-1}(\phigi-\magi\fixed_effect)
 -\frac{ N_{tot}+Nd}{2}\log(2\pi)
\end{eqnarray*}
where $N_{tot}=\sum_{i=1}^{N}n_i$ is the total number of observations.

First, consider a constant residual error model ($g=a$). The set of
parameters to estimate is $\theta=(\fixed_effect, \IIV, \ag)$. Then,
the complete model belongs to the exponential family and the
approximation step reduces to only updating the sufficient
statistics of the complete model:
\begin{eqnarray*}
s_{1,i,k} &= & s_{1,i,k-1}  + \gamma_k \left(  \phig_{i,k} - s_{1,i,k-1}   \right) , \hspace{1em}i=1,\ldots,N \\
 s_{2,k} &= & s_{2,k-1}  + \gamma_k \left( \sum_{i=1}^{N} \phig_{i,k} \, \phig_{i,k}^\prime - s_{2,k-1}   \right)  \\
 s_{3,k} &= & s_{3,k-1}  + \gamma_k \left( \sum_{i,j} \left( y_{ij} - f(x_{ij},\psigki) \right) ^2      - s_{3,k-1} \right) .
% s_{3,k} &= & s_{3,k-1}  + \gamma_k \left( S_{3,k-1}   - s_{3,k-1} \right) .
\end{eqnarray*}
Then, $\theta_{k+1}$ is obtained in the maximization step as follows:
 \begin{eqnarray}
\mugkun &= &\left( \sum_{i=1}^{N}  \magi^\prime \IIVk ^{-1} \magi\right)^{-1} \sum_{i=1}^N \magi^\prime \IIVk ^{-1} s_{1,i,k}  \\
\IIVkun &=& \frac{1}{N} \left(s_{2,k}- \sum_{i=1}^N(\magi\mugkun)s_{1,i,k}^\prime -\sum_{i=1}^N s_{1,i,k}(\magi\mugkun)^\prime +\sum_{i=1}^N (\magi\mugkun)(\magi\mugkun)^\prime\right) \label{react_gamma} \\
\ag_{k+1} & = & \sqrt{ \frac{s_{3,k}}{N_{tot}} }
\end{eqnarray}


\noindent {\bf Remark 1:  } The sequence of step sizes used in \monolix decreases as $k^{-a}$. More
precisely, for any sequence of integers $K_1,K_2,\ldots,K_J$ and any sequence $a_1,a_2,\ldots,a_J$
of real numbers such that $0\leq a_1 <a_2<\ldots<a_J\leq 1$, we define the sequence of step sizes
$(\gamma_k)$ as follows:
\begin{equation} \label{stepsize1}
\gamma_k = \frac{1}{k^{a_1}} \quad \mbox{for any } 1\leq k \leq K_1
\end{equation}
and for $2\leq j \leq J$,
\begin{equation} \label{stepsize2}
\gamma_k = \frac{1}{\left(k - K_{j-1}+\gamma_{K_{j-1}}^{-1/a_j}\right) ^{a_j}} \quad \mbox{for any
} \sum_{i=1}^{j-1} K_i +1\leq k \leq \sum_{i=1}^{j} K_i
\end{equation}
Here, $K=\sum_{j=1}^{J}K_j$ is the total number of iterations.

We recommend to use $a_1=0$ (that is $\gamma_k=1$) during the first iterations, and $a_J=1$ during
the last iterations. Indeed, the initial guess  $\theta_0$ may  be far  from the  maximum
likelihood value  we are looking for and the first iterations with $\gamma_k=1$ allow to converge
quickly to a neighborhood of the maximum likelihood estimator. Then, smaller step sizes ensure the
almost sure convergence of the algorithm to the maximum likelihood estimator.


\frame{\parbox{15cm} {\vspace*{.2cm} \quad In the case where $J=2$ with $a_1=0$ and $a_2=1$, the
sequence of step sizes  is
\begin{eqnarray*}
\gamma_k &=& 1  \quad  \quad \quad \quad  \mbox{ for } 1\leq k \leq K_1 \\
 &=& \frac{1}{k-K_1+1}  \quad \mbox{ for } K_1+1 \leq  k \leq K_1+K_2
\end{eqnarray*}
}}


\noindent {\bf Remark 2:  } The estimated covariance matrix $\IIVkun$ defined in
(\ref{react_gamma}) is a full covariance matrix. However, the covariance matrix $\IIV$ of the
random effects can have any covariance structure. If we assume, for example, that there is no
correlation between the random effects, we will set to 0 the non diagonal elements of $\IIVkun$
defined in (\ref{react_gamma}).

We can also assume that a random effect has no variance. If the $\ell$th random effect has a variance equal to 0, then the $\ell$th individual parameter is
no longer random and the simulation step of SAEM needs some modification. During the first $K_0$ iterations, we use SAEM as it was described above,
considering that all the effects are random and assuming that there is no correlation between the $\ell$th random effect and the other ones
($\omega^2_{\ell \ell^\prime}=0$ for any $\ell \neq \ell^\prime$). Then, during the next iterations, we use again SAEM, but the variance of this random effect is no longer
estimated: it is forced to decrease at each iteration by setting
\begin{equation}
\omega^2_{\ell\ell,k+1}= \alpha \ {\omega^2_{\ell\ell,k}} \quad , \quad K_0 \leq k \leq K
\end{equation}
where $\alpha$ is chosen between 0 and 1 such that $\omega^2_{\ell\ell,K}= 10^{-6}\omega^2_{\ell\ell,K_0}$.

\noindent {\bf Remark 3:} - For a  residual variance model of the form $g=  \bg \, f^\cg$, where
$\cg$ is fixed, the complete model also belongs to the exponential family and the estimation of
$\bg$ is straightforward:  the sufficient statistics sequence $(s_{3,k})$ is defined by
$$ s_{3,k} =  s_{3,k-1}  + \gamma_k \left(
\sum_{i,j} \left( \frac{y_{ij} - f(x_{ij},\psigki)}{f^\cg (x_{ij},\psigki)}  \right) ^2  - s_{3,k-1}
\right) $$ and $\bg_{k+1} =  \sqrt{ s_{3,k}/ N_{tot} }$.

- For a general residual variance model $g= \ag + \bg \, f^\cg$, the complete model does not belong
to the exponential family and the estimates of the residual variance parameters $(\ag,\bg,\cg)$
cannot be expressed as a function of some sufficient statistics. Then, let $(\Ag_k,\Bg_k,\Cg_k)$
that minimize the complete log-likelihood:
$$(A_k, B_k,C_k) = {\rm Arg}\min_{(\ag,\bg,\cg)}
\left\{ \sum_{i,j}\log( \ag + \bg f^{\cg}(x_{ij},\psigki) ) +\frac{1}{2} \sum_{i,j}\left(
\frac{y_{ij} - f(x_{ij},\psigki)}{\ag + \bg f^{\cg}(x_{ij},\psigki) } \right)^2 \right\}
$$
We update the residual variance parameters as follows:
 \begin{eqnarray}
\ag_{k+1} & = & \ag_{k} +  \gamma_k \left(  A_k - \ag_{k}  \right)   \\
\bg_{k+1} & = & \bg_{k} +  \gamma_k \left(  B_k - \bg_{k}  \right)   \\
\cg_{k+1} & = & \cg_{k} +  \gamma_k \left(  C_k - \cg_{k}  \right)
\end{eqnarray}
The estimation of $\fixed_effect$ and $\IIV$ remains unchanged.


\subsection{The MCMC-SAEM algorithm}
For model (1.1), the simulation step cannot be directly performed. Kuhn and Lavielle \cite{kuhn01}
propose to combine the SAEM algorithm with a MCMC (Markov Chain Monte Carlo) procedure. This procedure
consists in replacing the Simulation-step at iteration $k$ by $m$ iterations of the
Hastings-Metropolis algorithm.

Here, we will consider the Gaussian parameters $(\phigi)$. For $i=1,2,\ldots,N$
\begin{itemize}
\item let $\phig_{i,0}=\phig_{i}^{(k-1)}$
\item for $p=1,2,\ldots,m$,
\begin{enumerate}
\item draw $ \tilde{\phig}_{i,p}$ using the proposal kernel
$ q_{\theta_k}( \phig_{i,p-1} ,\cdot) $
\item set $ \phig_{i,p} =  \tilde{\phig}_{i,p} $ with probability
$$ \alpha( \phig_{i,p-1} ,  \tilde{\phig}_{i,p} ) = \min \left(1, \frac
{ p( \tilde{\phig}_{i,p} |y_i;\theta_k) q_{\theta_k}(\tilde{\phig}_{i,p} , \phig_{i,p-1} )}
{ p( \phig_{i,p-1} |y_i;\theta_k) q_{\theta_k}(\phig_{i,p-1} ,\tilde{\phig}_{i,p} )} \right)$$ and
$\phig_{i,p} =  \phig_{i,p-1}$ with probability $1- \alpha( \phig_{i,p-1} ,  \tilde{\phig}_{i,p} ) $.
\end{enumerate}
\item let $\phig_i^{(k)} =  \phig_{i,m}$.
 \end{itemize}

Several transition kernels, associated to different proposals can be successively used.
We use the four following proposal kernels:

\begin{enumerate}
\item $q_{\theta_k}^{(1)}$  is the prior distribution of $\phig_i$ at iteration $k$, that is the Gaussian distribution \\
${\cal N}(C_i\fixed_effect_k,\IIV_k)$ and then
$$ \alpha( \phig_{i,p-1} ,  \tilde{\phig}_{i,p} ) = \min \left(1, \frac{ p( y_i|\tilde{\phig}_{i,p};\theta_k) } { p( y_i|\phig_{i,p-1};\theta_k) } \right)$$
\item $q_{\theta_k}^{(2)}$ is a random permutation of the $\phig_i$: generate a random permutation $\sigma$ of $\{1,2,\ldots,N\}$ and
set $\tilde{\phig}_{i,p}=\phig_{\sigma(i),p-1}$.
\item $q_{\theta_k}^{(3)}$ is the multidimensional random walk ${\cal N}( \phig_{i,p-1} , \kappa\IIV_k)$. This kernel is symmetric and then
$$ \alpha( \phig_{i,p-1} ,  \tilde{\phig}_{i,p} ) = \min \left(1, \frac{ p( y_i,\tilde{\phig}_{i,p};\theta_k) } { p( y_i,\phig_{i,p-1};\theta_k) } \right)$$
\item $q_{\theta_k}^{(4)}$ is a succession of $d$ unidimensional Gaussian random walks: each component of $\phig_i$ are successively updated.
\end{enumerate}
Then, the simulation-step at iteration $k$ consists in running $m_1$ iterations of the Hasting-Metropolis with proposal $q_{\theta_k}^{(1)}$, $m_2$
iterations with proposal $q_{\theta_k}^{(2)}$, $m_3$ iterations with proposal $q_{\theta_k}^{(3)}$ and $m_4$ iterations with proposal
$q_{\theta_k}^{(4)}$.


\noindent {\bf Remark 1 : \ } During the first $K_b$ iterations (``burning'' iterations) of SAEM, we only run the MCMC algorithm but the parameters
are not updated.

 \noindent {\bf Remark 2 : \ } When the number $N$ of subjects is small, convergence of the algorithm can be improved by
running $L$ Markov Chain instead of only one. The simulation step requires to draw $L$ sequences $ { \phig^{(k,1)}} ,\ldots , { \phig^{(k,L)}} $ at
iteration $k$ and to combine stochastic approximation and Monte Carlo in the approximation step:
\begin{equation} \label{approx2}
 Q_k(\theta) = Q_{k-1}(\theta) + \gamma_k \left( \frac{1}{L}\sum_{\ell=1}^{L} \log p(\yg, { \phig^{(k,\ell)}} ;\theta) - Q_{k-1}(\theta) \right)
\end{equation}


\subsection{The Simulated Annealing SAEM algorithm}
Convergence of SAEM can strongly depend on the initial guess if the likelihood $\ell$ possesses
several local maxima. The Simulated Annealing version of SAEM improves the convergence of the
algorithm toward the global maximum of $\ell$.

For the sake of simplicity, we will consider here a constant residual error model $g=\ag$. Let
$$ U(\yg,\phig;\theta) = \frac{1}{2\ag^2} \sum_{i,j}\left(y_{ij} - f(x_{ij},h(\phigi)) \right)^2
+\frac{1}{2}\sum_{i=1}^{N}(\phigi-\magi\fixed_effect)^\prime \IIV^{-1}(\phigi-\magi\fixed_effect)$$ Then, we can write the complete likelihood:
\begin{eqnarray*}
p(\yg,\phig;\theta)  =  C(\theta)\, e^{-U(\yg,\phig;\theta)}
\end{eqnarray*}
where $C(\theta)$ is a normalizing constant that only depends on $\theta$.

For any {\it temperature} $T\geq0$, we consider the complete model
\begin{eqnarray*}
p_T(\yg,\phig;\theta)  =  C_T(\theta)\, e^{-\frac{1}{T}U(\yg,\phig;\theta)}
\end{eqnarray*}
where $C_T(\theta)$ is a normalizing constant. This model consists in replacing the variance matrix
$\IIV$ by $T\IIV$ and the residual variance $\ag^2$ by $T\ag^2$. In other words, a model ``with
a large temperature'' is a model with large variances.

We introduce a decreasing temperature sequence $(T_k, 1\leq k \leq K)$ and use the MCMC-SAEM
algorithm considering the complete model $p_{T_k}(\yg,\phig;\theta)$  at iteration $k$ (while the
usual version of MCMC-SAEM uses $T_k=1$ at each iteration). The sequence $(T_k)$ is large during
the first iterations and decreases to 1 with exponential rate. This is done by choosing large
initial variances $\IIV_0$ and $\ag^2_0$ and setting
\begin{eqnarray}
 \tilde{\IIV}_{k+1} &=& \frac{1}{N} \left(s_{2,k}- \sum_{i=1}^N(\magi\mugkun)s_{1,i,k}^\prime -\sum_{i=1}^N s_{1,i,k}(\magi\mugkun)^\prime +\sum_{i=1}^N (\magi\mugkun)(\magi\mugkun)^\prime\right)  \\
\ag_{k+1} & = & \sqrt{ \frac{s_{3,k}}{N_{tot}} }\\
\IIV_{k+1} &=& \max\left( \tau \IIV_{k}  \ , \ \tilde{\IIV}_{k+1}  \right)  \label{tau1} \\
\ag^2_{k+1} & = &  \max\left( \tau \ag^2_{k} \ , \  \frac{s_{3,k}}{N} \right) \label{tau2}
\end{eqnarray}
%\begin{eqnarray}
% \IIV_k  & = &  \tau \IIV_{k-1} ,  \label{tau1} \\
%\ag^2_k & = & \tau \ag^2_{k-1} \label{tau2}
%\end{eqnarray}
during the first iterations of the algorithm and where $0\leq\tau\leq 1$.

These large values of the variances make the conditional distribution $p(\phi|y;\theta )$ less
concentrated around its mode. This procedure allows the sequence $(\theta_k)$ to escape from the
local maxima of the likelihood and to converge to a neighborhood of the global maximum of $\ell$.
After that, the usual MCMC-SAEM algorithm is used, estimating the variances at each iteration.


\noindent {\bf Remark 1:} The Simulated Annealing version of SAEM is performed during the first $K_{sa}$ iterations. Of course, SAEM without any
simulated annealing can be run by setting $\tau=0$. On the other hand, simulated annealing is obtained with $\tau$ close to 1.

\noindent {\bf Remark 2:} We can use two different coefficients
$\tau_1$ and $\tau_2$ for $\IIV$ and $\ag^2$ in \monolix. It is
possible, for example, to choose $\tau_1<1$ and $\tau_2>1$, with a
small initial residual variance and large initial inter-subject
variances. In this case, SAEM tries to obtain the best possible fit
during the first iterations, allowing a large inter-subject
variability. During the next iterations, this variability is reduced
and the residual variance increases until reaching the best possible
trade-off between these two criteria.

\section{Some extensions}
\subsection{Model with BLQ data} \label{section_methodo_blq}

The model is described \hyperref[section_model_blq]{Section~\ref*{section_model_blq}}.

The maximum likelihood estimation is based on the log-likelihood function $L(y^{obs} \,; \, \theta)$ of the response $y^{obs}$ with
$\theta=(\fixed_effect,\IIV, \ag,\bg,\cg )$ the vector of all the parameters of the model
\begin{equation}
L(y^{obs} \,; \,\theta )=%\log \left( \prod_{i=1}^{N} p(y_i ; \theta\,)\right)=
\log \left( \prod_{i=1}^{N} \int p(y_i^{obs} ,y_i^{cens},\phig_i;\theta) \, d\phig_i\,
dy_i^{cens}\right),\label{lik}
\end{equation}
where $p(y_i^{obs} ,y_i^{cens},\phig_i;\,\theta\,)$ is the likelihood of the complete data
$(y_i^{obs},y_i^{cens},  \phig_i)$ of the $i$-th subject. The complete likelihood of the $i$-th
subject is equal to:
$$p(y_i^{obs},y_i^{cens},\phig_i;\theta)= \prod_{(i,j) \in I_{obs} } p(y_{ij}^{obs}|\phig_i;\theta)p(\phig_i;\theta)\prod_{(i,j)
\in I_{cens} } p(y_{ij}^{cens}|\phig_i;\theta)p(\phig_i;\theta),
$$
with
\begin{equation*}
\begin{array}{ccc}
p(y_{ij}^{obs}|\phig_i;\theta)=\pi(y_{ij}^{obs};f(\phig_i,t_{ij}),\sigma^2g^2(\phig_i,t_{ij}))\; \mathbbm{1}_{y_{ij}\geq LOQ},
 &\mbox{if } & (i,j) \in I_{obs} \hspace{1em}\text{and} \\
p(y_{ij}^{cens}|\phig_i;\theta)= \pi(y_{ij}^{cens};f(\phig_i,t_{ij}),\sigma^2g^2(\phig_i,t_{ij}))\;
\mathbbm{1}_{y_{ij}\leq LOQ}, & \mbox{if } & (i,j) \in I_{cens}  ,
\end{array}
\end{equation*}
where $\pi(x;m,v)$ is the probability density function of the Gaussian distribution with mean $m$
and variance $v$, evaluated at $x$.

Samson {\it et al.} proposed in \cite{samson_csda06}  an extension of the SAEM algorithm to handle left-censored data in NLMEM as an exact Maximum
Likelihood estimation method. The simulation of the censored data with a truncated Gaussian distribution is included in the MCMC procedure. The
convergence of this extended SAEM algorithm is proved under general conditions.

How  \monolix handles BLQ data is described \hyperref[section_feature_blq]{Section~\ref*{section_feature_blq}}.

\subsection{Estimation with a prior distribution} \label{section_methodo_prior}

A prior distribution on the fixed effect parameter
$\fixed_effect$ in the SAEM algorithm can be incorporated.

The parameter $\fixed_effect$ is considered as a random Gaussian variable. Let us denote $\meanprior$ the mean of this prior distribution and
$\varprior$ its diagonal variance matrix:
\begin{equation} \label{prior_mu}
\fixed_effect \sim {\cal N}(\meanprior,\varprior)
\end{equation}
The parameters $\meanprior$ and $\varprior$ are fixed.

Let $\thetaprior = (\meanprior,\varprior,\IIV,\xi)$ and $\dprior$ be the length of $\meanprior$. Then, the complete log-likelihood of $(\yg,\psig,\fixed_effect)$ can be written
\begin{eqnarray*}
\log p(\yg,\phig,\mu;\thetaprior) & = &  - \sum_{i,j}\log( g(x_{ij},h(\phigi);\xi) )
-\frac{1}{2} \sum_{i,j}\left( \frac{y_{ij} - f(x_{ij},h(\phigi))}{g(x_{ij},h(\phigi);\xi) } \right)^2 \\
& & -\frac{N}{2} \log (|\IIV|)
-\frac{1}{2}\sum_{i=1}^{N}(\phigi-\magi\fixed_effect)^\prime \IIV^{-1}(\phigi-\magi\fixed_effect) \\
&&-\frac{1}{2}\log(|\varprior|)-\frac{1}{2}(\fixed_effect-\meanprior)^\prime \varprior^{-1}(\fixed_effect-\meanprior) -\frac{
N_{tot}+Nd+\dprior}{2}\log(2\pi)
\end{eqnarray*}

The simulation step and the approximation step are similar to the ones of section \ref{saem}. Then,
$\mugkun$ is obtained in the maximization step as follows:
 \begin{eqnarray*}
\mugkun &= &\left( \varprior^{-1}+\sum_{i=1}^{N}  \magi^\prime \IIVk^{-1} \magi \right)^{-1}\left(\varprior^{-1}\meanprior+
\sum_{i=1}^N \magi^\prime \IIVk ^{-1} s_{1,i,k} \right) \\
\end{eqnarray*}
while $\IIV_{k+1}$ and $\sigma^2_{k+1}$ are computed as before.


Of course, it is possible to combine several methods to estimate the
complete set of parameters: we can fix some parameters, use maximum
likelihood estimation for some other parameters and  use prior
information for the other ones.

\subsection{Modeling the inter-occasion variability} \label{section_methodo_iov}
Mixed effects models with IOV are described \hyperref[section_model_iov]{Section~\ref*{section_model_iov}}.

An extension of the SAEM algorithm for models with two levels of
random effects can be found in \cite{PS08}. The methodology proposed
in this paper is limited to only two periods and assumes IOV on each
parameter ($\Gamma$ is a diagonal matrix with non zero element on
the diagonal). We have extended this methodology to any number of
occasions and also to any structure of the IOV covariance matrix
$\Gamma$. Furthermore, extension to any number of levels of IOV is available starting with \monolix~3.2.

Modelling IOV with \monolix is described \hyperref[section_feature_iov]{Section~\ref*{section_feature_iov}}.


\section{Estimation of the Fisher Information matrix} \label{sec_fish} Let $\thes$ be the true unknown value of $\theta$, and let $\hthetag$ be the
maximum likelihood estimate of $\theta$. If the observed likelihood function $\ell$ is sufficiently smooth, asymptotic theory for maximum-likelihood
estimation holds and

\begin{equation}
\sqrt{N}(\hthetag-\thes) \limite{N\to \infty}{} {\mathcal N}(0,I(\thes)^{-1})
\end{equation}
where $I(\thes)=- \DDt \log \ell(y;\thes)$ is the true Fisher information matrix. Thus, an
estimate of the asymptotic covariance of $\hthetag$ is the  inverse of the Fisher
information matrix $I(\hthetag)=- \DDt \log \ell(y;\hthetag)$.


\subsection{Linearization of the model}
The Fisher information matrix of the nonlinear mixed effects model defined in (1) cannot be computed in a closed-form.

An alternative is  to approximate this information matrix  by the Fisher information matrix of the Gaussian model deduced from the nonlinear mixed
effects model after linearization of the function $f$ around the conditional expectation of the individual Gaussian parameters $(\esp{\phi_{i}|y;\hat{\theta}}, 1\leq
i \leq N) $. The Fisher information matrix of this Gaussian model is a block matrix (no correlations between the estimated fixed effects and the
estimated variances). The gradient of $f$ is numerically computed.

\noindent {\bf Remark 1: } We do not recommend the linearization of
the model to estimate the parameters of the model, as it is done
with the FO and FOCE algorithms. On the other hand, many numerical
experiments have shown that this approach can be used to estimate
the Fisher information matrix.

\noindent {\bf Remark 2: } Obviously, this approach cannot be used with discrete data models\ldots

\subsection{A stochastic approximation of the Fisher Information Matrix}
It is possible to obtain an estimation of the Fisher information matrix using the Louis's missing
information principle \cite{loui82}:

\begin{equation}\label{louis}
\DDt \log \ell(y;\theta) = \mathrm{E}\big(\DDt \log p(y,\phig; \theta) | y ; \theta \big) +
\mathrm{Cov}\big(\Dt \log p(y,\phig;\theta)| y ; \theta \big)
\end{equation}
where
\begin{align*} \quad
\mathrm{Cov}\big(\Dt \log p(y,\phig;\theta)| y ; \theta \big) &=
\mathrm{E} \big(\Dt \log p(y,\phig; \theta) \Dt \log p(y,\phig; \theta)^\prime |y ; \theta \big)  \\
& -
\mathrm{E} \big( \Dt \log p(y,\phig; \theta) |y ; \theta \big)\mathrm{E} \big( \Dt \log p(y,\phig; \theta) |y ; \theta \big)^\prime
\end{align*}
and
$$\Dt \log g(y;\theta) = \mathrm{E} \big( \Dt \log p(y,\phig; \theta) |y ; \theta \big)$$
Here, $\Dt u$ is the gradient of $u$ ({\it i.e.} the vector of first derivatives of $u$ with respect to $\theta$) and  $\DDt u$ is the hessian of $u$ ({\it i.e.} the matrix of second derivatives of $u$ with respect to $\theta$).

Then, using SAEM, the matrix $\DDt \log \ell (y;\hthetag)$
can be approximated by the sequence $(H_k)$ defined as follows:
\begin{align*}
\Delta_k&=\Delta_{k-1}+\gamma_k\left(\partial_{\theta}\log
f(y,\phi_k;\theta_k)-\Delta_{k-1}\right)\\
D_k  &=   D_{k-1}   +   \gamma_k  \left( \DDt \log  f (y,  \phi_k;
    \theta_k)   - D_{k-1} \right) \\
G_k  &=   G_{k-1}   +   \gamma_k  \left(  \Dt \log f(y, \phi_k; \theta_k) \Dt \log f(y, \phi_k; \theta_k)^t - G_{k-1} \right) \\
H_{k} &= D_k + G_k - \Delta_k \Delta_k^t
\end{align*}

\section{Estimation of the individual parameters} \label{section_indivparam}

When the parameters of the model have been estimated, we can estimate the individual parameters $(\psigi)$. To do that, we will estimate the individual normally distributed parameters $(\phigi)$ and derive the estimates of $(\psigi)$ using the transformation $\psigi= h(\psigi)$.

Let $\hat{\theta}$ be the estimated value of $\theta$ computed with the SAEM algorithm and let $p(\phigi|y_i;\hat{\theta})$ be the conditional
distribution of $\phigi$ for $1\leq i \leq N$.

We use the MCMC procedure used in the SAEM algorithm to estimate these conditional distributions. More precisely, for $1\leq i \leq
N$, we empirically estimate:

\begin{itemize}
  \item the  conditional mode (or Maximum A Posteriori)
  $ m(\phigi|y_i;\hat{\theta})={\rm Arg}\max_\phigi p(\phigi|y_i;\hat{\theta})$,
  \item the conditional mean
  $ E(\phigi|y_i;\hat{\theta})$,
  \item the conditional standard deviation
 $ sd(\phigi|y_i;\hat{\theta})$.
 \end{itemize}

 \noindent{\bf Remarks:}
 \begin{enumerate}
  \item The prior distribution of $\phigi$ is a normal distribution, but not the conditional distribution $p(\phigi|y_i;\hat{\theta})$ (remember that the structural model is not a linear function of $\phigi$\ldots). Then, the conditional mode $m(\phigi|y_i;\hat{\theta})$ and the conditional expectation $ E(\phigi|y_i;\hat{\theta})$ are two different predictors of $\phigi$.
      \item If the transformation $h$ is not linear,
      \begin{eqnarray*}
       \esp{\psigi|y_i;\hat{\theta}} &=&  \esp{h(\phigi|y_i;\hat{\theta}} \\
       &\neq& h\left(\esp{\phigi|y_i;\hat{\theta}}  \right)
       \end{eqnarray*}
       In \monolix, we estimate $\esp{\phigi|y_i;\hat{\theta}}$ and $\esp{\psigi|y_i;\hat{\theta}}$.
\end{enumerate}

The number of iterations of the MCMC algorithm used to estimate the
conditional mean and standard deviation is adaptively chosen as
follows:
\begin{enumerate}
  \item The $(\phig_i)$ are initialized with the last value obtained in SAEM
  \item We run the Hastings-Metropolis with kernel $q^{(1)}$, $q^{(3)}$ and $q^{(4)}$ and compute at each iteration the empirical conditional mean
  and s.d. of $\phig_i$:
  \begin{eqnarray}
  e_{i,K} &= &\frac{1}{K}\sum_{k=1}^K \phig_{i,k} \\
  sd_{i,K} &= &\sqrt{\frac{1}{K}\sum_{k=1}^K \phig_{i,k}^2 - e_{i,K}^2 }
  \end{eqnarray}
  where $\phig_{i,k}$ is the value of $\phig_i$ at iteration $k$ of the MCMC algorithm.
  \item we stop the algorithm at iteration $K$ and use $e_{i,K}$ and $sd_{i,K}$ to estimate the conditional mean and s.d. of $\phig_i$ if, for
  any $ K-L_{mcmc}+1 \leq k \leq K$,
  \begin{eqnarray}
  % \nonumber to remove numbering (before each equation)
  \label{Lmcmc}  (1-\rho_{mcmc})\bar{e}_K & \leq & \bar{e}_k \leq  (1+\rho_{mcmc})\bar{e}{K} \\
  \nonumber  (1-\rho_{mcmc})\bar{sd}_{K} & \leq & \bar{sd}_{k} \leq  (1+\rho_{mcmc})\bar{sd}_{K}
  \end{eqnarray}
where $0<\rho_{mcmc}<1$. That means that the sequence of empirical means and s.d. must stay in a $\rho_{mcmc}$-confidence interval during $L_{mcmc}$
iterations.
\end{enumerate}

\section{Estimation of the likelihood} \label{estilik}
\subsection{Linearization of the model}
The likelihood of the nonlinear mixed effects model defined in (1) cannot be computed in a closed-form.

An alternative is  to approximate this likelihood  by the likelihood of the Gaussian model deduced from the nonlinear mixed
effects model after linearization of the function $f$ around
the predictions of the individual parameters $(\phigi, 1\leq i \leq N) $.
%the conditional expectation of the individual parameters $(\esp{\phig_{i}|y;\hat{\theta}}, 1\leq i \leq N) $.

\subsection{Estimation using importance sampling}

The likelihood of the observations can be estimated without any
approximation using a Monte-Carlo approach.
The likelihood $\ell$ of the observations can be decomposed as follows
\begin{eqnarray*}
 \ell(\yg;\thetag) &=& \int p(\yg,\phig;\thetag)\,d\phig \\
&=& \int h(\yg|\phig;\thetag)\pi(\phig;\thetag)\,d\phig
\end{eqnarray*}
where $\pi$ is the so-called {\it prior distribution} of $\phig$. According to (\ref{prior}), $\pi$
is a Gaussian distribution.

For any distribution $\tilde{\pi}$ absolutely continuous with respect to the prior distribution
$\pi$, we can write
$$
\ell(\yg;\thetag) = \int h(\yg|\phig;\thetag) \frac {\pi(\phig;\thetag)}{
\tilde{\pi}(\phig;\thetag) }  \tilde{\pi}(\phig;\thetag) \,d\phig
$$


Then, $\ell(\yg;\thetag)$ can be approximated via an {\it Importance Sampling} integration method:
\begin{enumerate}
\item draw $\phig^{(1)} ,\phig^{(2)} ,\ldots,\phig^{(M)} $ with the distribution $\tilde{\pi}(\cdot;\thetag)$,
\item let
\begin{equation} \label{islike}
 \ell_M(\yg;\thetag) =\frac{1}{M} \sum_{j=1}^{M} h(y|\phig^{(j)};\thetag )\frac{\pi( \phig^{(j)} ;\thetag)}{ \tilde{\pi}(\phig^{(j)} ;\thetag) }
\end{equation}
\end{enumerate}

The statistical properties of the estimator $\ell_M(\yg;\thetag)$ of the likelihood
$\ell(\yg;\thetag)$ strongly depend on the sampling distribution  $\tilde{\pi}$. First, note that
\begin{eqnarray*}
\esp{{\ell}_M(\yg;\thetag)} &=& \ell(\yg;\thetag), \\
\var{{\ell}_M(\yg;\thetag)} &=& {\cal O}(1/M).
\end{eqnarray*}
Furthermore, if $\tilde{\pi}$ is the conditional distribution
$p(\phi|\yg;\thetag)$, the variance of the estimator is null and
$\hat{\ell}_M(\yg;\thetag) = \ell(\yg;\thetag)$ for any value of
$M$. That means that an accurate estimation of $\ell(\yg;\thetag)$
can be obtained with a small value of $M$ if the sampling
distribution is close to the conditional distribution
$p(\phi|\yg;\thetag)$.


In \monolix, for $i=1,2,\ldots, N$, we empirically estimate the
conditional mean $\esp{\phigi|y_{i};\hthetag}$ and the conditional
variance $\var{\phigi|y_{i};\hthetag}$ of $\phigi$ as described
above. Then, the $\phigi^{(j)}$ are drawn with the sampling
distribution $\tilde{\pi}$ as follows:
$$\phigi^{(j)} = \esp{\phigi|y_{i};\hthetag} + \var{\phigi|y_{i};\hthetag}^{\frac{1}{2}} \times T_{ij}$$
where $(T_{ij})$ is a sequence of {\it i.i.d.} random variables distributed with a $t-$distribution with $\nu$ degrees of freedom.

It is possible to use the default value $\nu=5$. It is also possible
to automatically test different d.f in $\{2, 5, 10, 20\}$ and to
select the one that provides the smallest empirical variance for
$\ell_M(\yg;\thetag)$.


\section{Estimation of the weighted residuals} \label{section_wres}
\subsection{The Population Weighted Residuals}
The Population Weighted Residuals are evaluated as
   $$PWRES_{ij} = \frac{y_{ij} - \hat{y}^{pop}_{ij}}{\hat{\sigma}^{pop}_{ij}}$$
where $\hat{y}^{pop}_{ij}$ is the population prediction of $y_{ij}$ and $(\hat{\sigma}^{pop}_{ij})^2=Var_{\hat{\theta}}(y_{ij})$ is the variance of $y_{ij}$.

Two population predictions are proposed in \monolix:
\begin{enumerate}
  \item the {pop. param. prediction} $f\left(x_{ij} ; h\left( \mathbb{E}_{\hat{\theta}}(\phigi) \right) \right)= f(x_{ij} ; h(C_i \hat{\fixed_effect}))$
  \item the {pop. mean prediction}  $\mathbb{E}_{\hat{\theta}}(f(x_{ij} ; \psigi) ))=\mathbb{E}_{\hat{\theta}}(f(x_{ij} ; h(\phigi) ))$.
\end{enumerate}

Here,  $\mathbb{E}_{\hat{\theta}}(f(x_{ij} ; h(\phigi)  )$ and
$Var_{\hat{\theta}}(y_{ij})$ are estimated with a Monte-Carlo procedure.

\subsection{The Individual Weighted Residuals}
The Individual Weighted Residuals are evaluated as
$$IWRES_{ij} = \frac{y_{ij} - \hat{y}^{ind}_{ij}} {\hat{\sigma}^{ind}_{ij}}$$
where  $\hat{y}^{ind}_{ij} = f(x_{ij} ; \hat{\psig}_i )$ is the individual prediction of $y_{ij}$ and $(\hat{\sigma}^{ind}_{ij})^2 = g(x_{ij} ; \hat{\phig}_i,\hat{\xi})^2$ is the residual variance of $y_{ij}$.

The $\hat{\psig}_i$'s are the individual estimates of the ${\psig}_i$'s described Section~\ref{section_indivparam} (the conditional modes or the conditional means)


\noindent {\bf Remark:} When a transformed residual error model is used (an exponential error model for instance), the weighted residuals are computed using $t(y)$ instead of $y$.

\subsection{The Normalized Prediction Distribution }
The Normalized Prediction Distribution Errors are defined as follow
$$NPDE_{ij}=\Phi^{-1}(\hat{p}_{ij})$$
where $\Phi$ is the $\Nr(0,1)$ cumulative distribution function and where $\hat{p}_{ij}$ is an empirical estimator of
$$p_{ij} = \Prob{Y_{ij}<y_{ij}}$$
obtained by Monte-Carlo.


\section{Inputs and outputs}
\subsection{The inputs}

To summarize, \monolix requires to define the model and to fix some parameters used for the
algorithms. First, it is necessary to define:
\begin{itemize}
\item the structural model, that is the regression function $f$ defined in (\ref{nlme}),
\item the covariate model, that is the structure of the matrix $\fixed_effect$ defined in (\ref{prior}) and the covariates $(\covariate_i)$.
\item the variance-covariance model for the random effects, that is the structure of the variance-covariance matrix $\IIV$
defined in (\ref{prior}).
\item the residual variance model, that is the regression function $g$.
\end{itemize}

Then, it is necessary to specify several parameters for running the algorithms:
\begin{itemize}
\item the SAEM algorithm requires to specify
\begin{itemize}
\item the initial values of the fixed effects $\fixed_effect_0$, the initial variance-covariance matrix $\IIV_0$ of the
random effects and the initial residual variance coefficients $\ag_0$, $\bg_0$ and $\cg_0$,
\item the sequence of step sizes $(\gamma_k)$, that is the numbers of iterations $(K_1,K_2)$ and the coefficients
  $(a_1,a_2)$ defined in (\ref{stepsize1}) and (\ref{stepsize2}),
%\item the number of iterations $K_0$ used to estimate non random individual parameters.
\item the number of burning iterations $K_b$ used with the same value $\theta_0$ before updating the sequence $(\theta_k)$.
\end{itemize}
\item the MCMC algorithm requires to set
\begin{itemize}
\item the number of Markov Chains $L$,
\item the numbers $m_1$, $m_2$, $m_3$ and $m_4$ of iterations of the Hasting-Metropolis algorithm,
\item the probability of acceptance $\rho$ for  kernel $q^{(3)}$ and $q^{(4)}$,
\end{itemize}
\item the algorithm to estimate the conditional distribution of the $(\phig_i)$ requires to set
\begin{itemize}
\item the width of the confidence interval $\rho_{mcmc}$ (see (\ref{Lmcmc}),
\item the number of iterations $L_{mcmc}$.
\end{itemize}
\item the Simulated Annealing algorithm requires to set
\begin{itemize}
\item the coefficient $\tau_1$ and $\tau_2$ defining the decrease of the temperature (see (\ref{tau1},\ref{tau2}))
\item the number of iterations $K_{sa}$.
\end{itemize}
\item the Importance Sampling algorithm requires to set
\begin{itemize}
\item the Monte Carlo number $M$ used to estimate the observed likelihood (see (\ref{islike})).
\end{itemize}
\end{itemize}

\subsection{The outputs}

\noindent {\bf a) Estimation of the parameters:}

The SAEM algorithm computes the maximum likelihood estimate $\hthetag$ and estimates its covariance
matrix $I({\hthetag})^{-1}/N$ defined Section~\ref{sec_fish}.

Recall that $d$ is the number of individual parameters, then for $j=1,2\ldots d$, we can
\begin{enumerate}
\item estimate the vector of fixed effects $\fixed_effect$ (intercept and coefficients of the covariates) by $(\hat{\fixed_effect})$,
\item estimate the standard errors of $\fixed_effect$,
\item test if some components of $\fixed_effect$ are null by computing the significance level of the Wald test.
\end{enumerate}


Let $\IIV=(\omega_{jl}, 1\leq j,l \leq d)$. Then, for any $j,l=1,2\ldots d$, we can
\begin{enumerate}
\item estimate $\omega_{jl}$ by $\homega_{jl}$, for all $1\leq j,l \leq d$,
\item estimate the standard error of $\homega_{jl}$, for all $1\leq j,l \leq d$,
%\item test ``$\omega_{jl}=0$", by computing the significance level of the Wald test.
\end{enumerate}
%(In this version of \monolix, the standard errors of the non diagonal elements $\homega_{jl}$, with $j\neq l$, are not computed and the Wald test is only performed for the diagonal elements of $\IIV$).

\noindent {\bf b) Estimation of the conditional distributions:}

The MCMC algorithm provides an estimation of the conditional means, conditional modes and conditional standard deviations of the individual
parameters and of the random effects.

\noindent {\bf c) Estimation of the likelihood:}

The Importance Sampling algorithm computes an estimate $ \ell_M(\yg;\hthetag)$ of the observed likelihood together with its standard error.

\noindent {\bf d) Hypothesis testing and model selection:}

We can test the covariate model, the covariance model and the residual error model.

 The
AIC and BIC criteria are defined by
\begin{eqnarray}
AIC &=& - 2 \log \ell_M(\yg;\hthetag) + 2 P \\
BIC &=& - 2 \log \ell_M(\yg;\hthetag) + \log(N) P
\end{eqnarray}
where $P$ is the total number of parameters to be estimated and $N$ is the number of subjects.

When comparing two nested models ${\cal M}_0$ and ${\cal M}_1$ with dimensions $P_0$ and $P_1$ (with $P_1>P_0$), the Likelihood Ratio Test uses the
test statistic
$$LRT = 2 ( \log \ell_{M,1}(\yg;\hthetag_1) -  \log \ell_{M,0}(\yg;\hthetag_0) )$$
According to the hypotheses to test, the limiting distribution of $LRT$ under the null hypothesis is either a $\chi ^2$ distribution, or a mixture of a $\chi^2$ distribution and a
$\delta-Dirac$ distribution. For example:
\begin{itemize}
\item[-] to test whether some fixed effects are null, assuming the same covariance structure of the random effects, one should use
$$LRT \limite{N\to \infty}{} \chi^2(P_1-P_0) $$
\item[-] to test whether some correlations of the covariance matrix $\IIV$ are null, assuming the
same covariate model, one should use 
$$LRT \limite{N\to \infty}{} \chi^2(P_1-P_0) $$
\item[-] to test whether the variance of one of the random effects is zero, assuming the
same covariate model, one should use
$$LRT \limite{N\to \infty}{} \frac{1}{2} \chi^2(1) + \frac{1}{2}\delta_0 $$
\end{itemize}

\noindent {\bf e) Estimation of the weighted residuals:}

 The Population Weighted Residuals $(PWRES_{ij})$,  the Individual Weighted Residuals $(IWRES_{ij})$
 and the Normalized Prediction Distribution Errors $(NPDE_{ij})$ are computed as described Section~\ref{section_wres}
