\chapter{Introduction} \label{chapter_introduction}


\section{The objectives}
The objectives of \monolix are to perform:
\begin{enumerate}
\item Parameter estimation for nonlinear mixed effects models
\begin{itemize}
\item[-] computing the maximum likelihood estimator of the population parameters, without any approximation of the model
(linearization, quadrature approximation, \ldots), using the Stochastic Approximation Expectation
Maximization (SAEM) algorithm,
\item[-] computing standard errors for the maximum likelihood estimator
\item[-] computing the conditional modes, the conditional means and the conditional standard deviations of the individual parameters, using the Hastings-Metropolis algorithm
\end{itemize}
\item Model selection
\begin{itemize}
\item[-] comparing several models using some information criteria (AIC, BIC)
\item[-] testing hypotheses using the Likelihood Ratio Test
\item[-] testing parameters using the Wald Test
\end{itemize}
\item Goodness of fit plots
\item Data simulation.
\end{enumerate}
\monolix also handles a broad spectrum of models including models defined with differential equations, left censored data, discrete data models, repeated time to events, hidden Markov models, mixture models,\ldots

Theoretical  analysis of the algorithms used in this software can be found in \cite{marc, samson_jspi06, kuhn01, kl05}. Several application of SAEM
in agronomy \cite{mako06}, animal breeding \cite{Jaffrezic06} and PKPD analysis \cite{comets07, LM07, samson_csda06, samson_sim06a} have been
published by several members of the \monolix group. Several applications to PKPD analysis were also proposed during the last PAGE (Population
Approach Group in Europe) meetings (\cite{page06a, page05a, page03, page06b, page04a, page06c, page05b} as well as a comparison of estimation
algorithms \cite{page05c}, ({http://www.page-meeting.org}).


The aim of the present document is to help a \monolix beginner to
discover the software abilities. Section 1 presents the nonlinear
mixed effects models, Section 2 presents the algorithms used in this
software and Section 3 explains how to use \monolix and its graphical
interface. In this section, most features of the \monolix interface
are illustrated with the pharmacokinetic theophylline example.
Advanced Features and examples are detailed in Section 4.
Section 5 explains how to write a new model with MLXTRAN or with
Matlab.



\section{The nonlinear mixed effects model}

Detailed and complete presentations of the nonlinear mixed effects model can be found in \cite{davgil2, davgil1, PB00}. See also the many references
therein.

We consider the following general nonlinear mixed effects model for continuous outputs:
\begin{equation}
\label{nlme}
y_{ij}=f(x_{ij},\psigi)+ g(x_{ij},\psigi,\xi)\varepsilon_{ij} \ \ , \ 1\leq i \leq N \ \ ,
\ \ 1 \leq j \leq n_i
\end{equation}
Here,
\begin{itemize}
\item $y_{ij}\in \Rset$ is the $j$th observation of subject $i$,
\item $N$ is the number of subjects,
\item $n_i$ is the number of observations of subject $i$,
\item the regression variables, or design variables, ($x_{ij}$) are assumed to be known, $x_{ij}\in
\Rset^\nreg$,
\item for subject $i$, the vector $\psigi=(\psigil \, ; \, 1\leq \ell \leq \npsi) \in \Rset^\npsi$ is a vector of $\npsi$ individual parameters:
\begin{equation} \label{prior}
\psigi=H(\fixed_effect,\covariate_i,\etagi)
\end{equation}
where
\begin{itemize}
\item $\covariate_i =(\covariate_{im} \, ; \, 1\leq m \leq \ncov)$ is a known vector of $\ncov$ covariates,
\item $\fixed_effect$ is an unknown vector of fixed effects of size $\nbeta$,
\item $\etagi$ is an unknown vector of normally distributed random effects of size $\neta$:
$$\etagi \sim_{i.i.d.} {\cal N}(0,\IIV)$$
\end{itemize}

\item the residual errors $(\varepsilon_{ij})$  are random variables with mean zero and variance 1,
\item the residual error model is defined by the function $g$ and some parameters $\xi$.
\end{itemize}


Here, the parameters of the model are $\thetag=(\fixed_effect,\IIV,\xi)$. We will denote
 $\ell(y;\theta)$ the likelihood of the observations $\yg =(y_{ij} \, ; \, 1  \leq i \leq n \ , 1
\leq j \leq n_i)$ and $p(\yg,\psig;\theta) $ the likelihood of the complete data $(\yg,\psig)= (y_{ij}, \psi_i \, ; \, 1
\leq i \leq n \ , 1 \leq j \leq n_i)$. Thus, $$\ell(y;\theta) = \int p(\yg,\psig;\theta)  \ d\psig .$$

Let us see now the statistical model used in \monolix~3.2 more in details.

\subsection{The statistical model for the individual parameters} \label{section_model_indiv}
In \monolix~3.2, we assume that $\psigi$ is a transformation of a Gaussian random vector $\phigi$:
\begin{equation} \label{prior2}
\psigi=h(\phigi)
\end{equation}
where, by rearranging the covariates $(\covariate_{im})$ into a matrix $\Covariate_i$, $\phigi$ can be written as
\begin{equation} \label{prior3}
\phigi=\Covariate_i \fixed_effect + \etagi
\end{equation}

\subsubsection{Examples of transformations}
Here, different transformations $(h_\ell)$ can be used for the different components of $\psigi=(\psigil)$ where $\psigil=h_\ell(\phigil)$ for $\ell=1, 2, \ldots , \ell$.
\begin{itemize}
\item $\psigil$ has a log-normal distribution if $h_\ell(u)=e^u$,
\item assuming that $\psigil$ takes its values in $(0,1)$, we can use a logit transformation $h_\ell(u)=1/(1+e^{-u})$, or a probit transformation $h_\ell(u)=\Prob{{\cal N}(0,1)\leq u}$.
\item assuming that $\psigil$ takes its values in $(A,B)$, we can define $h_\ell(u)=A + (B-A)/(1+e^{-u})$, or  $h_\ell(u)=A + (B-A)\Prob{{\cal N}(0,1)\leq u}$.
    \end{itemize}

In the following, we will use either the parameters $\psigi$ or the Gaussian transformed parameters $\phigi=h^{-1}(\psigi)$.

The model can address continuous and/or categorical covariates.


\subsubsection{Example of continuous covariate model} \label{section_model_contcov}
Consider a PK model that depends on volume and clearance and consider the following covariate model for these two parameters:
\begin{eqnarray*}
CL_i & = & CL_{\rm pop}\left(\frac{W_i}{W_{\rm pop}}\right)^{\beta_{CL,W}} \left(\frac{A_i}{A_{\rm pop}}\right)^{\beta_{CL,A}} e^{\eta_{i,1}} \\
V_i & = & V_{\rm pop}\left(\frac{W_i}{W_{\rm pop}}\right)^{\beta_{V,W}}  e^{\eta_{i,2}}
\end{eqnarray*}
Where $W_i$ and $A_i$ are the weight and the age of subjet $i$ and where $W_{\rm pop}$ and $A_{\rm pop}$ are some ``typical'' values of these two covariates in the population. Here,
$\psigi$ will denote the PK parameters (clearance and volume) of subject $i$ and
$\phigi$ its log-clearance and log-volume.
Let
$$W^\star_i = \log\left(\frac{W_i}{W_{\rm pop}}\right) \quad ; \quad A^\star_i = \log\left(\frac{A_i}{A_{\rm pop}}\right) $$
Then,
\begin{eqnarray*}
\phigi &= & \left( \begin{array}{c}  \log(CL_i) \\  \log(V_i) \\ \end{array} \right) \\
&=& \left( \begin{array}{ccccc}  1 & 0 & W^\star_i & W^\star_i & 0 \\ 0 & 1 & 0 & 0 & W^\star_i \\ \end{array} \right)
\left( \begin{array}{c}  \log(CL_{\rm pop}) \\  \log(V_{\rm pop}) \\ \beta_{CL,W} \\ \beta_{CL,A} \\ \beta_{V,W} ) \\ \end{array} \right)
+  \left( \begin{array}{c}  \eta_{i,1} \\  \eta_{i,2} \\ \end{array} \right) \\
&=& \Covariate_i \fixed_effect + \etagi
\end{eqnarray*}

\subsubsection{Example of categorical covariate model} \label{section_model_catcov}
Assume that some categorical covariate  $G_i$ takes the values 1, 2, \ldots, $K$.
Assume that if patient $i$ belongs to group $k$, {\it i.e.} $G_i=k$, then
\begin{eqnarray*}
\log(CL_i) &=&  \log(CL_{{\rm pop},k}) + \eta_i
\end{eqnarray*}
where $CL_{{\rm pop},k}$ is the population clearance in group $k$.

Let $k^\star$ be the reference group. Then, for any group $k$, we will decompose the population clearance $CL_{{\rm pop},k}$ as
\begin{eqnarray*}
\log(CL_{{\rm pop},k}) = \log(CL_{{\rm pop},k^\star}) + \beta_k
\end{eqnarray*}
where $\beta_{k^\star} = 0$.

The variance of the random effects can also depend on this categorical covariate:
$$ \etagi \sim \mathcal{N}(0, \IIV_k) \ \ \ {\rm if } \ \ G_i=k $$

\noindent {\bf Remark:} It is assumed in \monolix~3.2 that the correlation matrix of the random effect is the same for all the groups.
In other words, only the variances of the random effects can  differ from one group to another.

\lignem{Choice of the transformation is described  \hyperref[section_using_individual]{Section~\ref*{section_using_individual}}. \\
Selection of the covariate model  is described  \hyperref[section_using_covariate]{Section~\ref*{section_using_covariate}}. \\
Examples with categorical covariates are given \hyperref[section_feature_catcov]{Section~\ref*{section_feature_catcov}}.
}

\subsection{The residual error model} \label{section_model_residual}
The within-group errors ($\varepsilon_{ij}$)  are  supposed  to be  Gaussian random variables  with mean  zero and variance $1$.
Furthermore, we suppose that the $\varepsilon_{ij}$ and the $\eta_{i}$ are mutually independent.

Different error models can be used in \monolix~3.2:
\begin{itemize}
\item the constant error model assumes that $g=a$ and $\xi=a$,
\item the proportional error model assumes that $g=b\,f$ and $\xi=b$,
\item a combined error model assumes that $g=a+b\,f$ and $\xi=(a,b)$,
\item an alternative combined error model assumes that $g=\sqrt{a^2+b^2\,f^2}$ and $\xi=(a,b)$,
\item a combined error model with power assumes that $g=a+b\,f^c$ and $\xi=(a,b,c)$,
\item \ldots
\end{itemize}

Furthermore, all these error models can be applied to some transformation of the data:
\begin{eqnarray}\label{def_t}
t(y_{ij})=t(f(x_{ij},\psigi))+ g(x_{ij},\psigi,\xi)\varepsilon_{ij}
\end{eqnarray}

For example:
\begin{itemize}
\item the exponential error model assumes that $y>0$:
\begin{eqnarray*}
t(y) &=& \log(y) \\
y&=&f e^{g\varepsilon}
\end{eqnarray*}
\item the logit error model assumes that $0<y<1$:
\begin{eqnarray*}
t(y)&=&\log(y/(1-y)) \\
 y&=&\frac{f}{f+(1-f) e^{-g\varepsilon}}
\end{eqnarray*}
\item the logit error model can be extended if we assume that $A<y<B$:
\begin{eqnarray*}
 t(y)&=&\log((y-A)/(B-y)) \\
y&=&A+(B-A)\frac{f-A}{f-A+(B-f) e^{-g\varepsilon}}
\end{eqnarray*}
\end{itemize}

\noindent It is possible with \monolix to assume that the residual errors ($\varepsilon_{ij}$) are correlated:
\begin{equation} \label{autocorr}
{\rm corr}(\varepsilon_{i,j},\varepsilon_{i,j+1}) = \rho^{(x_{i,j+1}-x_{i,j})}
\end{equation}
Here, we assume that $0\leq \rho <1$ and that for any $i$,
$(x_{i,j}, 1\leq j \leq n_i)$ is an increasing sequence of
regression scalar variables.


\lignem{Selection of the residual error model is described  \hyperref[section_using_residual]{Section~\ref*{section_using_residual}}. \\
Several examples of residual error models are provided with the demos:   \\[-0.5cm]
\begin{itemize}
\item  combined error model: {\texttt warfarin/warfarin\_PK\_project}
\item  exponential error model: {\texttt PK/Bolus1cptMM\_project}
\item extended logit error model: {\texttt error model/Imax\_errorband\_project}
\item autocorrelated residual errors: {\texttt error model/infusion\_correrror\_project} \\[-0.5cm]
\end{itemize}
. \vspace*{-0.5cm}
}


\subsection{Multi-responses model} \label{section_model_multi}
The basic model can be extended to multi-responses:
\begin{eqnarray*}
y_{ij}^{(1)} &=& f_1(x_{ij}^{(1)} ,\psi_i)+ g_1(x_{ij}^{(1)} ,\psi_i;\xi_1)\varepsilon_{ij}^{(1)}  \ \ , \ 1\leq i \leq N \ \ , \ \ 1 \leq j \leq n_{i1} \\
\vdots &  & \vdots \\
y_{ij}^{(L)} &=& f_L(x_{ij}^{(L)} ,\psi_i)+ g_L(x_{ij}^{(L)} ,\psi_i;\xi_L)\varepsilon_{ij}^{(L)}  \ \ ,
\ 1\leq i \leq N \ \ , \ \ 1 \leq j \leq n_{iL}
\end{eqnarray*}
This is useful, for example, for PKPD models in which the input of
the PD model $x_{ij}^{(2)}$ is the concentration, that is the output
of the PK model $f_1(x_{ij}^{(1)} ,\psigi)$.

\lignem{How  to handle models with multiple outputs is described \hyperref[section_feature_pkpd]{Section~\ref*{section_feature_pkpd}}. \\
Several examples are provided using the warfarin data: \\ \texttt{warfarin\_PKPD1\_project, warfarin\_PKPD2\_project,
warfarin\_PKPD3\_project, warfarin\_PKPD4\_project}.
}


\subsection{Model with BLQ data} \label{section_model_blq}

In some context, because of assay limitation, when data $y_{ij}$ are inferior to a limit of quantification
($LOQ$), we do not observe $y_{ij}$ but only the censored value $LOQ$.    These data are usually named BLQ (Below the Limit of Quantification) data or left-censored data.
%In some other context,   data are censored when they are superior to LOQ. These data are thus called right-censored data.

Let denote $I_{obs}=\{(i,j)|y_{ij}\geq LOQ\}$  and $I_{cens}=\{(i,j)|y_{ij}\leq LOQ\}$ the index
sets of  the uncensored and censored observations respectively. For $(i,j) \in I_{cens} $, let
$y_{ij}^{cens}=y_{ij} $ denote the unknown value of the censored observation $j$ of subject $i$.
Let denote $y_{i}^{cens}$ the vector of censored observations of subject $i$.
 Finally, we observe
\begin{equation*}
y_{ij}^{obs} = \left\{
\begin{array}{ccc}
y_{ij} & \mbox{ if  } & (i,j) \in I_{obs}, \\
LOQ & \mbox{ if } &(i,j) \in I_{cens}.
\end{array}
\right.
\end{equation*}
We denote $y_{i}^{obs}=(y_{i1}^{obs},\ldots,y_{in_i}^{obs})$ as the observations of subject $i$ and
$y^{obs}=(y_{1}^{obs},\ldots,y_{N}^{obs})$ the total observations dataset.

\lignem{Methodology for BLQ data is described \hyperref[section_methodo_blq]{Section~\ref*{section_methodo_blq}}. \\
How  \monolix handles BLQ data is described \hyperref[section_feature_blq]{Section~\ref*{section_feature_blq}}.}

\subsection{Modeling the inter-occasion variability} \label{section_model_iov}

We will denote $y_{ikj}$ the $j$th observation for subject $i$ during occasion $k$:
\begin{equation}
y_{ikj}= f(\psig_{ik},t_{ikj})+g(\psig_{ik},t_{ikj},\xi)\varepsilon_{ikj}
\end{equation}
Here, $\psig_{ik}=h(\phig_{ik})$ is the individual parameter of subject $i$ at occasion $k$:
\begin{equation}
\phig_{ik}=C_{ik} \, \fixed_effect +\eta_{i}+\kappa_{ik}
\end{equation}
\begin{itemize}
\item[-] $C_{ik}$ is the matrix of covariates of subject $i$ at occasion $k$,
\item[-] $\eta_i$ random effect of subject $i$ (inter-subject variability): $ \eta_i \sim \mathcal{N}(0,\Omega)$,
\item[-] $\kappa_{ik}$  random effect of subject $i$ at occasion $k$ (inter-occasion variability): $ \kappa_{ik} \sim \mathcal{N}(0,\Gamma)$,
\item[-] $\eta_i$ and $\kappa_{ik}$ are assumed to be independent,
\item[-] $\Omega$   inter-subject variability covariance matrix,
\item[-] $\Gamma$    inter-occasion variability covariance matrix.
\end{itemize}



\lignem{Methodology for IOV is described \hyperref[section_methodo_iov]{Section~\ref*{section_methodo_iov}}. \\
How to model IOV with \monolix is described \hyperref[section_feature_iov]{Section~\ref*{section_feature_iov}}.}


\subsection{Discrete data models} \label{section_model_discrete}
The basic model proposed in (\ref{nlme}) is a regression model used for fitting continuous data that can be extended for  categorical data or count data models.
Assume that $(y_{ij})$ takes its values in $\{0, 1, 2,\ldots\}$. We define the conditional likelihood of the observations using a mixed effects model:
\begin{equation}
\label{discrete_nlme}
\Prob{y_{ij}=k | \psigi} = f(k,x_{ij},\psigi) \ \ , \ 1\leq i \leq N \ \ ,
\ \ 1 \leq j \leq n_i
\end{equation}
In other words, for any $i$, the probability that $y_{ij}$ takes the value $k$ depends on some (unknown) individual parameter $\psigi$ and possibly on some (known) design variable $x_{ij}$.

A mixed hidden Markov models (mixed HMM, or MHMM) assumes that there exists some non observed sequences $(z_{ij})$ (the states) that take their values in ${1,2,\ldots L}$ such that, for any $i$,
\begin{itemize}
\item $(z_{ij}, j\geq 1)$ is a Markov Chain,
\item conditionally to the sequence of states $(z_{ij})$, the $(y_{ij})$ are independent random variables
\item the transition probabilities $\Prob{z_{i,j+1}=v | z_{ij}=u}$ and the emission probabilities ({\it i.e.} conditional probabilities) $\Prob{y_{ij}=k | z_{ij}=u}$ depend on some individual parameters $\psigi$.
\end{itemize}

\lignem{How to model categorical data, count data and HMM is described respectively
Sections \hyperref[section_feature_categorical]{Section~\ref*{section_feature_categorical}},
\hyperref[section_feature_count]{Section~\ref*{section_feature_count}} and
\hyperref[section_feature_hmm]{Section~\ref*{section_feature_hmm}}.
}

\subsection{Mixture models and model mixtures} \label{section_model_mixture}
\subsubsection{Mixture models}
In \monolix, a mixture model assume that there exist some ``latent'' categorical covariate  $G$ that takes $K$ values.
Then, the mixture model reduces to the categorical covariate model described \hyperref[section_model_catcov]{Section~\ref*{section_model_indiv}} but
here, the categorical covariates are unknown: they are treated as random variables and the probabilities
$$\pi_k = \Prob{G_i=k}$$
are part of the statistical model and should be estimated as well.

\subsubsection{Model mixtures}
Let $f_1,f_2,\ldots f_K$ be $K$ different structural models,
\begin{itemize}
\item {\bf Between Subject Model Mixture (BSMM)}

We assume that some categorical covariate  $G$ takes $K$ values and that
$$
y_{ij}=f_k(x_{ij},\psigi)+ \varepsilon_{ij}
\ \ ,
 \ \ \ {\rm if } \ \ G_i=k
 $$
In a BSMM model, the ``latent'' categorical covariates are unknown: they are treated as random variables and the probabilities
$$\pi_k = \Prob{G_i=k}$$
are part of the statistical model and should be estimated as well.
\item {\bf Within Subject Model Mixture (WSMM)}

For any patient $i$, let $p_{i,1}, p_{i,2}, \ldots , p_{i,K}$ be $K$ proportions such that
\begin{eqnarray*}
y_{ij} &=& f_i(x_{ij},\psigi)+ \varepsilon_{ij}  \\
f_i&=& p_{i,1}f_1 + p_{i,2}f_2 + \ldots + p_{i,K}f_K
\end{eqnarray*}
In a WSMM model, the proportions $(p_{i,k})$ are additional individual parameters that should be modeled as well (under the constraint that the sum is 1).
\end{itemize}

\lignem{How to use mixture models is described Section \hyperref[section_feature_mixmod]{Section~\ref*{section_feature_mixmod}}. \\
Examples of BSMM and WSMM with \monolix are presented Section \hyperref[section_feature_modmix]{Section~\ref*{section_feature_modmix}}.
}



